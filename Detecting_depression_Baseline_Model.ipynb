{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting depression Baseline Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfhUcum8yWjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d2e2485b-3ac0-4b55-db81-b702bd697aad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmPKuaUqeQ54",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1Hq-nddeM5Z",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsyWiBjouAS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6310ccc9-abd4-4ae1-94ba-368b4afb3b47"
      },
      "source": [
        "!pip install chart_studio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting chart_studio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/ce/330794a6b6ca4b9182c38fc69dd2a9cbff60fd49421cb8648ee5fee352dc/chart_studio-1.1.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from chart_studio) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from chart_studio) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from chart_studio) (1.12.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from chart_studio) (4.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->chart_studio) (3.0.4)\n",
            "Installing collected packages: chart-studio\n",
            "Successfully installed chart-studio-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU46_D3RpkKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.sparse import hstack\n",
        "from nltk import word_tokenize\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import itertools\n",
        "from scipy import stats\n",
        "from ast import literal_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAESJfKM54j8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9784c089-441a-45af-cb83-8d1d6367dd2a"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5OsnVhXcUji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a6e73ec1-89bb-4afe-ed03-00adb66cb3a3"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAMgk2YztzS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import json\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import keras.utils\n",
        "from keras import utils as np_utils\n",
        "\n",
        "#Keras Tokenizer just replaces certain punctuation characters and splits on the remaining space character.\n",
        "#NLTK Tokenizer uses the Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w89HBgNvbhHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh2ZvAITIO2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "WINDOWS_SIZE = 10\n",
        "labels=['none','mild','moderate','moderately severe', 'severe']\n",
        "num_classes = len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKST8w7Di96R",
        "colab_type": "text"
      },
      "source": [
        "### Creating a dataframe from the transcript file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtNdhS37uO9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transcripts_to_dataframe(directory):\n",
        "    rows_list = []\n",
        "        \n",
        "    filenames = os.listdir(directory)\n",
        "    \n",
        "    if \".DS_Store\" in filenames:\n",
        "        filenames.remove(\".DS_Store\")\n",
        "        \n",
        "    for filename in filenames:\n",
        "        transcript_path = os.path.join(directory, filename)\n",
        "        transcript = pd.read_csv(transcript_path, sep='\\t')\n",
        "        m = re.search(\"(\\d{3})_TRANSCRIPT.csv\", filename)\n",
        "        if m:\n",
        "            person_id = m.group(1)\n",
        "            p = {}\n",
        "            question = \"\"\n",
        "            answer = \"\"\n",
        "            lines = len(transcript)\n",
        "            for i in range(0, lines):\n",
        "                row = transcript.iloc[i]\n",
        "                if (row[\"speaker\"] == \"Ellie\") or (i == lines - 1):\n",
        "                    p[\"personId\"] = person_id\n",
        "                    if \"(\" in str(question):\n",
        "                        question = question[question.index(\"(\") + 1:question.index(\")\")]\n",
        "                    p[\"question\"] = question\n",
        "                    p[\"answer\"] = answer\n",
        "                    if question != \"\":\n",
        "                        rows_list.append(p)\n",
        "                    p = {}\n",
        "                    answer = \"\"\n",
        "                    question = row[\"value\"]\n",
        "                else:\n",
        "                    answer = str(answer) + \" \" + str(row[\"value\"])\n",
        "\n",
        "    all_participants = pd.DataFrame(rows_list, columns=['personId', 'question', 'answer'])\n",
        "    all_participants.to_csv(directory + 'all.csv', sep=',')\n",
        "    print(\"File was created\")\n",
        "    return all_participants"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2ZvtFYb0Ysv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e119f91a-e688-4c89-c341-ce4ccdb739fe"
      },
      "source": [
        "#loading the data\n",
        "data_path = \"/content/drive/My Drive/transcripts/\"\n",
        "all_participants = transcripts_to_dataframe(data_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File was created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMO4n5vR0nEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "becfde75-1bb2-4971-c233-16b9355fe7a4"
      },
      "source": [
        "all_participants.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>personId</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>hi i'm ellie thanks for coming in today</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>300</td>\n",
              "      <td>i was created to talk to people in a safe and ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>300</td>\n",
              "      <td>think of me as a friend i don't judge i can't ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>300</td>\n",
              "      <td>i'm here to learn about people and would love ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>300</td>\n",
              "      <td>i'll ask a few questions to get us started and...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>300</td>\n",
              "      <td>how are you doing today</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>300</td>\n",
              "      <td>that's good</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>300</td>\n",
              "      <td>where are you from originally</td>\n",
              "      <td>atlanta georgia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>300</td>\n",
              "      <td>really</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>300</td>\n",
              "      <td>why'd you move to l_a</td>\n",
              "      <td>um my parents are from here um</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300</td>\n",
              "      <td>how do you like l_a</td>\n",
              "      <td>i love it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300</td>\n",
              "      <td>what are some things you really like about l_a</td>\n",
              "      <td>i like the weather i like the opportunities u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300</td>\n",
              "      <td>how easy was it for you to get used to living ...</td>\n",
              "      <td>um it took a minute somewhat easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>300</td>\n",
              "      <td>what are some things you don't really like abo...</td>\n",
              "      <td>congestion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>300</td>\n",
              "      <td>mhm</td>\n",
              "      <td>that's it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>300</td>\n",
              "      <td>okay</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>300</td>\n",
              "      <td>what'd you study at school</td>\n",
              "      <td>um i took up business and administration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>300</td>\n",
              "      <td>cool</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>300</td>\n",
              "      <td>are you still doing that</td>\n",
              "      <td>uh yeah i am here and there i'm on a break ri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>300</td>\n",
              "      <td>what's your dream job</td>\n",
              "      <td>uh probably to open up my own business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   personId  ...                                             answer\n",
              "0       300  ...                                                   \n",
              "1       300  ...                                                   \n",
              "2       300  ...                                                   \n",
              "3       300  ...                                                   \n",
              "4       300  ...                                                   \n",
              "5       300  ...                                               good\n",
              "6       300  ...                                                   \n",
              "7       300  ...                                    atlanta georgia\n",
              "8       300  ...                                                   \n",
              "9       300  ...                     um my parents are from here um\n",
              "10      300  ...                                          i love it\n",
              "11      300  ...   i like the weather i like the opportunities u...\n",
              "12      300  ...                  um it took a minute somewhat easy\n",
              "13      300  ...                                         congestion\n",
              "14      300  ...                                          that's it\n",
              "15      300  ...                                                   \n",
              "16      300  ...           um i took up business and administration\n",
              "17      300  ...                                                   \n",
              "18      300  ...   uh yeah i am here and there i'm on a break ri...\n",
              "19      300  ...             uh probably to open up my own business\n",
              "\n",
              "[20 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1YNh1jN5fFs",
        "colab_type": "text"
      },
      "source": [
        "### Removing the stopwords and cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8muhU3MNGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "c13aa6e5-c9da-473f-cff6-892342ae2b16"
      },
      "source": [
        "#Selecting the data from the data from based on the questions asked  ['where', 'when', 'how', 'why', 'are', 'what', 'do', 'have', 'can', 'did', 'is', 'could', 'so', 'tell', 'who', 'has']\n",
        "interrogative = [\"where\", \"when\", \"how\",\"why\",\"are\",\"what\",\"do\",\"have\",\"can\",\"did\",\"is\", \"could\", \"so\", \"tell\", \"who\", \"has\"]\n",
        "rslt_df = all_participants[all_participants.question.str.contains('|'.join(interrogative),na=False)]\n",
        "rslt_df['answer'].replace('', np.nan, inplace=True)\n",
        "rslt_df.dropna(subset = [\"answer\"], inplace=True)\n",
        "rslt_df.reset_index(drop=True, inplace=True)\n",
        "rslt_df\n",
        "#rslt_df = rslt_df.to_csv(\"/content/drive/My Drive/rslt_df.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6746: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>personId</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>how are you doing today</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>300</td>\n",
              "      <td>where are you from originally</td>\n",
              "      <td>atlanta georgia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>300</td>\n",
              "      <td>why'd you move to l_a</td>\n",
              "      <td>um my parents are from here um</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>300</td>\n",
              "      <td>how do you like l_a</td>\n",
              "      <td>i love it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>300</td>\n",
              "      <td>what are some things you really like about l_a</td>\n",
              "      <td>i like the weather i like the opportunities u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8698</th>\n",
              "      <td>390</td>\n",
              "      <td>that's so good to hear</td>\n",
              "      <td>mm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8699</th>\n",
              "      <td>390</td>\n",
              "      <td>is there anything you regret</td>\n",
              "      <td>um hm no um except meeting that one woman uh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8700</th>\n",
              "      <td>390</td>\n",
              "      <td>what advice would you give to yourself ten or ...</td>\n",
              "      <td>uh i don't know probably try a little harder ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8701</th>\n",
              "      <td>390</td>\n",
              "      <td>tell me how you spend your ideal weekend</td>\n",
              "      <td>oh um getting out of town um going going away...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8702</th>\n",
              "      <td>390</td>\n",
              "      <td>what are you most proud of in your life</td>\n",
              "      <td>um well my daughter's a great source of pride...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8703 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     personId  ...                                             answer\n",
              "0         300  ...                                               good\n",
              "1         300  ...                                    atlanta georgia\n",
              "2         300  ...                     um my parents are from here um\n",
              "3         300  ...                                          i love it\n",
              "4         300  ...   i like the weather i like the opportunities u...\n",
              "...       ...  ...                                                ...\n",
              "8698      390  ...                                                 mm\n",
              "8699      390  ...       um hm no um except meeting that one woman uh\n",
              "8700      390  ...   uh i don't know probably try a little harder ...\n",
              "8701      390  ...   oh um getting out of town um going going away...\n",
              "8702      390  ...   um well my daughter's a great source of pride...\n",
              "\n",
              "[8703 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNh5CHOz3O6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
        "\n",
        "def text_to_wordlist(text, remove_stopwords=True, stem_words=False):    \n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = stopwords.words(\"english\")\n",
        "        text = [wordnet_lemmatizer.lemmatize(w) for w in text if not w in stops ]\n",
        "        text = [w for w in text if w != \"nan\" ]\n",
        "    else:\n",
        "        text = [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
        "        text = [w for w in text if w != \"nan\" ]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    \n",
        "    text = re.sub(r\"\\<\", \" \", text)\n",
        "    text = re.sub(r\"\\>\", \" \", text)\n",
        "    \n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY-vwQ-a5oRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a corpus with the words from the answers without stopwords given by the patients\n",
        "all_participants_mix = rslt_df.copy() # However, if you need the original list unchanged when the new list is modified, you can use copy() method. This is called shallow copy.\n",
        "all_participants_mix['clean_answer'] = all_participants_mix.apply(lambda row: text_to_wordlist(row.answer).split(), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xNLUgQj5xZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [w for w in all_participants_mix['clean_answer'].tolist()]\n",
        "words = set(itertools.chain(*words)) #chain('ABC', 'DEF') --> A B C D E F\n",
        "vocab_size = len(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvJU9CR8jPl0",
        "colab_type": "text"
      },
      "source": [
        "### Getting the top common words used by the patients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT52O9-p-Fel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dcdd5281-b9b6-4ebc-d996-093d3dafea38"
      },
      "source": [
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'luggage',\n",
              " 'reevaluate',\n",
              " 'thoughtful',\n",
              " 'dean',\n",
              " 'northern',\n",
              " 'intimate',\n",
              " 'overjoyously',\n",
              " 'molding',\n",
              " 'chance',\n",
              " 'lured',\n",
              " 'correctly',\n",
              " 'money',\n",
              " 'obsessive',\n",
              " 'invite',\n",
              " 'category',\n",
              " 'politely',\n",
              " 'ecology',\n",
              " 'got',\n",
              " 'esteem',\n",
              " 'practiced',\n",
              " 'started',\n",
              " 'permeable',\n",
              " 'bed',\n",
              " 'subtitle',\n",
              " 'wholeso',\n",
              " 'unsure',\n",
              " 'worm',\n",
              " 'common',\n",
              " 'wellbutrin',\n",
              " 'page',\n",
              " 'authentic',\n",
              " 'maintain',\n",
              " 'celebrate',\n",
              " 'infinite',\n",
              " 'g',\n",
              " 'bumped',\n",
              " 'tree',\n",
              " 'claim',\n",
              " 'mysef',\n",
              " 'deer',\n",
              " 'issue',\n",
              " 'har',\n",
              " 'dunno',\n",
              " 'provide',\n",
              " 'slightly',\n",
              " 'tendency',\n",
              " 'longest',\n",
              " 'let',\n",
              " 'protecting',\n",
              " 'diffferent',\n",
              " 'messing',\n",
              " 'sincere',\n",
              " 'sentence',\n",
              " 'violin',\n",
              " 'harm',\n",
              " 'quitter',\n",
              " 'degree',\n",
              " 'unprepared',\n",
              " 'mass',\n",
              " 'owned',\n",
              " 'ge',\n",
              " 'terrain',\n",
              " 'prominent',\n",
              " 'fascinated',\n",
              " 'burbank',\n",
              " 'limitation',\n",
              " 'school',\n",
              " 'prescribe',\n",
              " 'abroad',\n",
              " 'madagascar',\n",
              " 'psychotherapy',\n",
              " 'empathic',\n",
              " 'meantime',\n",
              " 'yawn',\n",
              " 'toolbox',\n",
              " 'conveyance',\n",
              " 'chain',\n",
              " 'defuse',\n",
              " 'programmed',\n",
              " 'soon',\n",
              " 'sluggish',\n",
              " 'sobbing',\n",
              " 'near',\n",
              " 'rage',\n",
              " 'meditate',\n",
              " 'idle',\n",
              " 'difficult',\n",
              " 'manner',\n",
              " 'looked',\n",
              " 'unfolding',\n",
              " 'forbid',\n",
              " 'evangelist',\n",
              " 'mumbles',\n",
              " 'hammock',\n",
              " 'dos',\n",
              " 'heal',\n",
              " 'america',\n",
              " 'wise',\n",
              " 'count',\n",
              " 'nominated',\n",
              " 'enrollment',\n",
              " 'filling',\n",
              " 'recharges',\n",
              " 'properly',\n",
              " 'speaks',\n",
              " 'turned',\n",
              " 'hurt',\n",
              " 'raw',\n",
              " 'grocery',\n",
              " 'provoking',\n",
              " 'handed',\n",
              " 'selfishness',\n",
              " 'smart',\n",
              " 'profitable',\n",
              " 'track',\n",
              " 'complained',\n",
              " 'fickle',\n",
              " 'dent',\n",
              " 'demand',\n",
              " 'trajectory',\n",
              " 'ups',\n",
              " 'overwhelming',\n",
              " 'butt',\n",
              " 'cared',\n",
              " 'clarify',\n",
              " 'discover',\n",
              " 'patient',\n",
              " 'mexican',\n",
              " 'cyclavia',\n",
              " 'seventy',\n",
              " 'double',\n",
              " 'tattoo',\n",
              " 'thrive',\n",
              " 'betrayed',\n",
              " 'glass',\n",
              " 'direction',\n",
              " 'gratification',\n",
              " 'current',\n",
              " 'sweaty',\n",
              " 'filmmaker',\n",
              " 'tony',\n",
              " 'tumbled',\n",
              " 'obispo',\n",
              " 'somthing',\n",
              " 'exception',\n",
              " 'enlightened',\n",
              " 'follow',\n",
              " 'unconditional',\n",
              " 'native',\n",
              " 'halibut',\n",
              " 'inform',\n",
              " 'express',\n",
              " 'don',\n",
              " 'cross',\n",
              " 'intership',\n",
              " 'sing',\n",
              " 'assume',\n",
              " 'potelingly',\n",
              " 'discussing',\n",
              " 'montgomery',\n",
              " 'position',\n",
              " 'recording',\n",
              " 'fantastic',\n",
              " 'deli',\n",
              " 'square',\n",
              " 'faster',\n",
              " 'outlive',\n",
              " 'breathe',\n",
              " 'deborah',\n",
              " 'affront',\n",
              " 'mild',\n",
              " 'woulda',\n",
              " 'adaptable',\n",
              " 'carlos',\n",
              " 'bunk',\n",
              " 'carpet',\n",
              " 'sent',\n",
              " 'impose',\n",
              " 'non',\n",
              " 'cured',\n",
              " 'sure',\n",
              " 'reading',\n",
              " 'temperment',\n",
              " 'crap',\n",
              " 'maui',\n",
              " 'recognize',\n",
              " 'clarity',\n",
              " 'depending',\n",
              " 'breathtakingly',\n",
              " 'powerful',\n",
              " 'ass',\n",
              " 'door',\n",
              " 'anal',\n",
              " 'belt',\n",
              " 'prevent',\n",
              " 'paying',\n",
              " 'blanket',\n",
              " 'columbia',\n",
              " 'solved',\n",
              " 'age',\n",
              " 'oxygen',\n",
              " 'dividing',\n",
              " 'st',\n",
              " 'functioning',\n",
              " 'knot',\n",
              " 'mortgage',\n",
              " 'cruising',\n",
              " 'winter',\n",
              " 'buddy',\n",
              " 'economical',\n",
              " 'calming',\n",
              " 'item',\n",
              " 'lifetime',\n",
              " 'package',\n",
              " 'created',\n",
              " 'fish',\n",
              " 'bathroom',\n",
              " 'stigma',\n",
              " 'remote',\n",
              " 'tuskegee',\n",
              " 'paid',\n",
              " 'allowed',\n",
              " 'utilize',\n",
              " 'wishing',\n",
              " 'development',\n",
              " 'retreat',\n",
              " 'remedy',\n",
              " 'unmotivated',\n",
              " 'indiana',\n",
              " 'ministry',\n",
              " 'physician',\n",
              " 'loner',\n",
              " 'body',\n",
              " 'belonging',\n",
              " 'two',\n",
              " 'clothes',\n",
              " 'deceased',\n",
              " 'spectacular',\n",
              " 'airport',\n",
              " 'individually',\n",
              " 'freelancer',\n",
              " 'unmanageable',\n",
              " 'connie',\n",
              " 'whole',\n",
              " 'keyboard',\n",
              " 'expensive',\n",
              " 'take',\n",
              " 'studying',\n",
              " 'bored',\n",
              " 'cannabis',\n",
              " 'practical',\n",
              " 'anemia',\n",
              " 'occasionally',\n",
              " 'horrified',\n",
              " 'animosity',\n",
              " 'ensenada',\n",
              " 'win',\n",
              " 'trap',\n",
              " 'lakers',\n",
              " 'fidgety',\n",
              " 'verge',\n",
              " 'shower',\n",
              " 'lighter',\n",
              " 'respirator',\n",
              " 'picture',\n",
              " 'diabetic',\n",
              " 'luh',\n",
              " 'lipstick',\n",
              " 'community',\n",
              " 'letter',\n",
              " 'staunch',\n",
              " 'distinct',\n",
              " 'disrespect',\n",
              " 'bud',\n",
              " 'insightful',\n",
              " 'traumatic',\n",
              " 'lemans',\n",
              " 'inspiration',\n",
              " 'deceive',\n",
              " 'pray',\n",
              " 'married',\n",
              " 'cooperation',\n",
              " 'battery',\n",
              " 'unlike',\n",
              " 'lord',\n",
              " 'worried',\n",
              " 'guilt',\n",
              " 'joshua',\n",
              " 'guessing',\n",
              " 'flew',\n",
              " 'bull',\n",
              " 'soaking',\n",
              " 'festival',\n",
              " 'kitchen',\n",
              " 'review',\n",
              " 'pancake',\n",
              " 'responding',\n",
              " 'pa',\n",
              " 'feedback',\n",
              " 'godparent',\n",
              " 'controlling',\n",
              " 'bodily',\n",
              " 'diverse',\n",
              " 'persistent',\n",
              " 'adventure',\n",
              " 'overhear',\n",
              " 'stormed',\n",
              " 'zombie',\n",
              " 'pr',\n",
              " 'explained',\n",
              " 'gamble',\n",
              " 'funding',\n",
              " 'flow',\n",
              " 'administration',\n",
              " 'critical',\n",
              " 'truant',\n",
              " 'notice',\n",
              " 'enforce',\n",
              " 'screenwriting',\n",
              " 'angelino',\n",
              " 'pinpoint',\n",
              " 'held',\n",
              " 'trainer',\n",
              " 'injustice',\n",
              " 'importance',\n",
              " 'work',\n",
              " 'expressing',\n",
              " 'charging',\n",
              " 'financially',\n",
              " 'well',\n",
              " 'stumped',\n",
              " 'spiritual',\n",
              " 'cul',\n",
              " 'influenced',\n",
              " 'aff',\n",
              " 'anthony',\n",
              " 'credit',\n",
              " 'cheating',\n",
              " 'accomplishment',\n",
              " 'skinnier',\n",
              " 'shocked',\n",
              " 'revert',\n",
              " 'overeating',\n",
              " 'demostrations',\n",
              " 'son',\n",
              " 'last',\n",
              " 'kayak',\n",
              " 'surface',\n",
              " 'golden',\n",
              " 'intoxicated',\n",
              " 'embrace',\n",
              " 'preparation',\n",
              " 'motivational',\n",
              " 'accusing',\n",
              " 'therapy',\n",
              " 'inherently',\n",
              " 'formless',\n",
              " 'underground',\n",
              " 'aggressively',\n",
              " 'cop',\n",
              " 'ticking',\n",
              " 'toward',\n",
              " 'became',\n",
              " 'solve',\n",
              " 'relaxing',\n",
              " 'magic',\n",
              " 'guitar',\n",
              " 'sweepstakes',\n",
              " 'faux',\n",
              " 'scatterbrained',\n",
              " 'knowledgeable',\n",
              " 'disturbance',\n",
              " 'lingering',\n",
              " 'whatsoever',\n",
              " 'regretted',\n",
              " 'casually',\n",
              " 'unique',\n",
              " 'worth',\n",
              " 'regionally',\n",
              " 'unproductive',\n",
              " 'destination',\n",
              " 'adapted',\n",
              " 'rela',\n",
              " 'future',\n",
              " 'calmly',\n",
              " 'pound',\n",
              " 'youngster',\n",
              " 'wellesley',\n",
              " 'dumbing',\n",
              " 'meditator',\n",
              " 'enjoy',\n",
              " 'dude',\n",
              " 'cheaper',\n",
              " 'sweat',\n",
              " 'considering',\n",
              " 'suffer',\n",
              " 'fatalistic',\n",
              " 'tone',\n",
              " 'inhale',\n",
              " 'intersection',\n",
              " 'requires',\n",
              " 'fa',\n",
              " 'smelled',\n",
              " 'generating',\n",
              " 'vice',\n",
              " 'brief',\n",
              " 'contentment',\n",
              " 'separated',\n",
              " 'ability',\n",
              " 'nobody',\n",
              " 'validate',\n",
              " 'ice',\n",
              " 'museum',\n",
              " 'wrongdoing',\n",
              " 'debt',\n",
              " 'refused',\n",
              " 'system',\n",
              " 'moving',\n",
              " 'successfully',\n",
              " 'located',\n",
              " 'rare',\n",
              " 'ellie',\n",
              " 'fierce',\n",
              " 'generation',\n",
              " 'dinosaur',\n",
              " 'ethiopia',\n",
              " 'cookbook',\n",
              " 'miss',\n",
              " 'yes',\n",
              " 'feasible',\n",
              " 'wimp',\n",
              " 'sodomized',\n",
              " 'transient',\n",
              " 'journalism',\n",
              " 'pilgrimmage',\n",
              " 'devastating',\n",
              " 'situatio',\n",
              " 'mantra',\n",
              " 'geography',\n",
              " 'shcool',\n",
              " 'roman',\n",
              " 'lashing',\n",
              " 'yell',\n",
              " 'disappear',\n",
              " 'quicker',\n",
              " 'trojan',\n",
              " 'landscape',\n",
              " 'within',\n",
              " 'discerning',\n",
              " 'welder',\n",
              " 'washington',\n",
              " 'single',\n",
              " 'screw',\n",
              " 'vibrant',\n",
              " 'lay',\n",
              " 'temp',\n",
              " 'german',\n",
              " 'supermarket',\n",
              " 'severely',\n",
              " 'juilliard',\n",
              " 'estranged',\n",
              " 'chinese',\n",
              " 'busyness',\n",
              " 'manage',\n",
              " 'curse',\n",
              " 'collection',\n",
              " 'maintainance',\n",
              " 'reserved',\n",
              " 'beaten',\n",
              " 'sun',\n",
              " 'doubt',\n",
              " 'studi',\n",
              " 'circle',\n",
              " 'dictated',\n",
              " 'dementia',\n",
              " 'magnificent',\n",
              " 'morbid',\n",
              " 'crawling',\n",
              " 'deed',\n",
              " 'dialogue',\n",
              " 'wrong',\n",
              " 'scientologist',\n",
              " 'hopefully',\n",
              " 'advisor',\n",
              " 'unpaved',\n",
              " 'adapt',\n",
              " 'appetite',\n",
              " 'cement',\n",
              " 'relation',\n",
              " 'someplace',\n",
              " 'pressure',\n",
              " 'popping',\n",
              " 'weariness',\n",
              " 'commended',\n",
              " 'meaningless',\n",
              " 'bugging',\n",
              " 'psychological',\n",
              " 'nation',\n",
              " 'noisy',\n",
              " 'to',\n",
              " 'minor',\n",
              " 'commandment',\n",
              " 'smaller',\n",
              " 'defeatist',\n",
              " 'deeply',\n",
              " 'bunch',\n",
              " 'mentioned',\n",
              " 'puddle',\n",
              " 'cordial',\n",
              " 'sugar',\n",
              " 'cultural',\n",
              " 'sow',\n",
              " 'delicate',\n",
              " 'understood',\n",
              " 'driver',\n",
              " 'conversing',\n",
              " 'philosphy',\n",
              " 'cool',\n",
              " 'indianapolis',\n",
              " 'conformist',\n",
              " 'inconsiderate',\n",
              " 'multiple',\n",
              " 'burden',\n",
              " 'bef',\n",
              " 'mm',\n",
              " 'fascination',\n",
              " 'redo',\n",
              " 'fanny',\n",
              " 'scuba',\n",
              " 'option',\n",
              " 'alter',\n",
              " 'line',\n",
              " 'soy',\n",
              " 'happiest',\n",
              " 'workout',\n",
              " 'flashy',\n",
              " 'eas',\n",
              " 'dishonest',\n",
              " 'almost',\n",
              " 'heaven',\n",
              " 'state',\n",
              " 'derelict',\n",
              " 'cat',\n",
              " 'sensitivity',\n",
              " 'definition',\n",
              " 'obvious',\n",
              " 'avoidance',\n",
              " 'nightmare',\n",
              " 'blue',\n",
              " 'practice',\n",
              " 'gold',\n",
              " 'wish',\n",
              " 'pending',\n",
              " 'metro',\n",
              " 'career',\n",
              " 'remembrance',\n",
              " 'throwing',\n",
              " 'pattern',\n",
              " 'bucking',\n",
              " 'interested',\n",
              " 'power',\n",
              " 'artificial',\n",
              " 'vow',\n",
              " 'bachelorhood',\n",
              " 'salton',\n",
              " 'pier',\n",
              " 'empathetic',\n",
              " 'jazz',\n",
              " 'fearless',\n",
              " 'niche',\n",
              " 'fresh',\n",
              " 'mitt',\n",
              " 'scrubbed',\n",
              " 'stranger',\n",
              " 'ordered',\n",
              " 'never',\n",
              " 'probably',\n",
              " 'crowded',\n",
              " 'gunpoint',\n",
              " 'authorization',\n",
              " 'bohemian',\n",
              " 'whoa',\n",
              " 'humanizing',\n",
              " 'punish',\n",
              " 'realize',\n",
              " 'bury',\n",
              " 'analyze',\n",
              " 'defeating',\n",
              " 'ecuadorian',\n",
              " 'kind',\n",
              " 'complicated',\n",
              " 'sober',\n",
              " 'trade',\n",
              " 'creating',\n",
              " 'rain',\n",
              " 'operation',\n",
              " 'andy',\n",
              " 'elafonisos',\n",
              " 'tuesday',\n",
              " 'blissed',\n",
              " 'computer',\n",
              " 'enjoyment',\n",
              " 'monetary',\n",
              " 'jackrabbit',\n",
              " 'scar',\n",
              " 'wanted',\n",
              " 'wife',\n",
              " 'hap',\n",
              " 'emplo',\n",
              " 'limited',\n",
              " 'thanks',\n",
              " 'weird',\n",
              " 'cheerful',\n",
              " 'bike',\n",
              " 'absorb',\n",
              " 'knack',\n",
              " 'berlin',\n",
              " 'plant',\n",
              " 'dichotomy',\n",
              " 'threshold',\n",
              " 'ended',\n",
              " 'simultaneously',\n",
              " 'prayer',\n",
              " 'separately',\n",
              " 'doctor',\n",
              " 'desk',\n",
              " 'imma',\n",
              " 'divorce',\n",
              " 'achieving',\n",
              " 'duke',\n",
              " 'family',\n",
              " 'necessarily',\n",
              " 'ear',\n",
              " 'song',\n",
              " 'primarily',\n",
              " 'ranger',\n",
              " 'somebody',\n",
              " 'harmful',\n",
              " 'captured',\n",
              " 'prima',\n",
              " 'pos',\n",
              " 'rooftop',\n",
              " 'burst',\n",
              " 'di',\n",
              " 'total',\n",
              " 'energizing',\n",
              " 'extravagant',\n",
              " 'played',\n",
              " 'concept',\n",
              " 'warhol',\n",
              " 'chocolate',\n",
              " 'client',\n",
              " 'bypass',\n",
              " 'die',\n",
              " 'may',\n",
              " 'fend',\n",
              " 'begin',\n",
              " 'keeping',\n",
              " 'occured',\n",
              " 'semiconductor',\n",
              " 'nonjudgmental',\n",
              " 'kill',\n",
              " 'paralleling',\n",
              " 'efficient',\n",
              " 'reorganizing',\n",
              " 'miniscule',\n",
              " 'filmmaking',\n",
              " 'knocked',\n",
              " 'demanding',\n",
              " 'see',\n",
              " 'creatively',\n",
              " 'forced',\n",
              " 'culturally',\n",
              " 'mentor',\n",
              " 'spot',\n",
              " 'orange',\n",
              " 'earning',\n",
              " 'mani',\n",
              " 'caregiver',\n",
              " 'trustworthiness',\n",
              " 'chemistry',\n",
              " 'monica',\n",
              " 'around',\n",
              " 'robbery',\n",
              " 'environment',\n",
              " 'compassion',\n",
              " 'sugarcoat',\n",
              " 'fashion',\n",
              " 'cauliflower',\n",
              " 'precedent',\n",
              " 'grouchy',\n",
              " 'hi',\n",
              " 'cent',\n",
              " 'sped',\n",
              " 'naval',\n",
              " 'attraction',\n",
              " 'shy',\n",
              " 'auth',\n",
              " 'secretly',\n",
              " 'stipend',\n",
              " 'oh',\n",
              " 'discipline',\n",
              " 'acclimated',\n",
              " 'crayola',\n",
              " 'setback',\n",
              " 'amsterdam',\n",
              " 'deprived',\n",
              " 'exposure',\n",
              " 'psychic',\n",
              " 'unacknowledged',\n",
              " 'lame',\n",
              " 'sightseeing',\n",
              " 'humming',\n",
              " 'guy',\n",
              " 'bustle',\n",
              " 'transit',\n",
              " 'wow',\n",
              " 'past',\n",
              " 'accountable',\n",
              " 'operator',\n",
              " 'mistreated',\n",
              " 'forgot',\n",
              " 'storm',\n",
              " 'trauma',\n",
              " 'genetics',\n",
              " 'litt',\n",
              " 'inhaling',\n",
              " 'conquer',\n",
              " 'face',\n",
              " 'rein',\n",
              " 'produce',\n",
              " 'manifestation',\n",
              " 'sketchy',\n",
              " 'thinking',\n",
              " 'five',\n",
              " 'friendly',\n",
              " 'session',\n",
              " 'psychology',\n",
              " 'contact',\n",
              " 'offer',\n",
              " 'logical',\n",
              " 'differently',\n",
              " 'restaurant',\n",
              " 'slow',\n",
              " 'throughout',\n",
              " 'faith',\n",
              " 'sensation',\n",
              " 'phy',\n",
              " 'marketing',\n",
              " 'commericalization',\n",
              " 'materialistic',\n",
              " 'toxin',\n",
              " 'healthcare',\n",
              " 'handedly',\n",
              " 'tr',\n",
              " 'teacup',\n",
              " 'twelve',\n",
              " 'humongous',\n",
              " 'midland',\n",
              " 'laugh',\n",
              " 'involved',\n",
              " 'evil',\n",
              " 'criticized',\n",
              " 'brat',\n",
              " 'specificity',\n",
              " 'lsat',\n",
              " 'motel',\n",
              " 'toughy',\n",
              " 'taped',\n",
              " 'expression',\n",
              " 'toughest',\n",
              " 'messed',\n",
              " 'traveled',\n",
              " 'damage',\n",
              " 'tah',\n",
              " 'redeemed',\n",
              " 'mature',\n",
              " 'ruminate',\n",
              " 'leader',\n",
              " 'interactive',\n",
              " 'putting',\n",
              " 'shallow',\n",
              " 'opinionated',\n",
              " 'socially',\n",
              " 'easter',\n",
              " 'undecided',\n",
              " 'bravo',\n",
              " 'passed',\n",
              " 'excruciating',\n",
              " 'element',\n",
              " 'brag',\n",
              " 'recall',\n",
              " 'written',\n",
              " 'wiser',\n",
              " 'jolt',\n",
              " 'affect',\n",
              " 'allow',\n",
              " 'fused',\n",
              " 'balding',\n",
              " 'vocal',\n",
              " 'agenda',\n",
              " 'brunt',\n",
              " 'confusion',\n",
              " 'contest',\n",
              " 'dirt',\n",
              " 'seize',\n",
              " 'accepting',\n",
              " 'eh',\n",
              " 'landlord',\n",
              " 'pretty',\n",
              " 'trader',\n",
              " 'deny',\n",
              " 'stopping',\n",
              " 'rica',\n",
              " 'feeding',\n",
              " 'ireland',\n",
              " 'repel',\n",
              " 'neighborhood',\n",
              " 'settleness',\n",
              " 'princess',\n",
              " 'tunnel',\n",
              " 'bothing',\n",
              " 'territory',\n",
              " 'mindful',\n",
              " 'reacted',\n",
              " 'grandparents',\n",
              " 'eighteen',\n",
              " 'inadvertent',\n",
              " 'awhile',\n",
              " 'feeled',\n",
              " 'concentrate',\n",
              " 'sucked',\n",
              " 'believing',\n",
              " 'cocktail',\n",
              " 'helping',\n",
              " 'candy',\n",
              " 'disagreeing',\n",
              " 'exploring',\n",
              " 'caused',\n",
              " 'gig',\n",
              " 'pillow',\n",
              " 'outdoor',\n",
              " 'pi',\n",
              " 'drain',\n",
              " 'seminar',\n",
              " 'pet',\n",
              " 'erase',\n",
              " 'rockstar',\n",
              " 'theme',\n",
              " 'survive',\n",
              " 'bikers',\n",
              " 'grounded',\n",
              " 'highway',\n",
              " 'anyways',\n",
              " 'electrifying',\n",
              " 'nurture',\n",
              " 'shopped',\n",
              " 'anyway',\n",
              " 'deciding',\n",
              " 'inv',\n",
              " 'afterwards',\n",
              " 'crazy',\n",
              " 'consumer',\n",
              " 'compass',\n",
              " 'am',\n",
              " 'result',\n",
              " 'al',\n",
              " 'objected',\n",
              " 'appearing',\n",
              " 'super',\n",
              " 'fiscal',\n",
              " 'budget',\n",
              " 'cosmetology',\n",
              " 'investment',\n",
              " 'bird',\n",
              " 'sigh',\n",
              " 'adjust',\n",
              " 'headed',\n",
              " 'vacation',\n",
              " 'roofing',\n",
              " 'avoid',\n",
              " 'disappoint',\n",
              " 'numbing',\n",
              " 'pft',\n",
              " 'cinematographer',\n",
              " 'naitivity',\n",
              " 'relative',\n",
              " 'proactive',\n",
              " 'designated',\n",
              " 'friendlier',\n",
              " 'famous',\n",
              " 'rebound',\n",
              " 'finely',\n",
              " 'smacked',\n",
              " 'lip',\n",
              " 'migrated',\n",
              " 'kne',\n",
              " 'all',\n",
              " 'hair',\n",
              " 'worrying',\n",
              " 'irratated',\n",
              " 'christian',\n",
              " 'steady',\n",
              " 'managing',\n",
              " 'lose',\n",
              " 'mo',\n",
              " 'learned',\n",
              " 'drunk',\n",
              " 'becoming',\n",
              " 'combine',\n",
              " 'arrived',\n",
              " 'conversation',\n",
              " 'others',\n",
              " 'harass',\n",
              " 'cooked',\n",
              " 'beginning',\n",
              " 'noir',\n",
              " 'property',\n",
              " 'relate',\n",
              " 'reacting',\n",
              " 'madly',\n",
              " 'repertoire',\n",
              " 'collecting',\n",
              " 'interviewing',\n",
              " 'light',\n",
              " 'prioritize',\n",
              " 'other',\n",
              " 'outspoken',\n",
              " 'humorous',\n",
              " 'program',\n",
              " 'ayelling',\n",
              " 'mem',\n",
              " 'documenting',\n",
              " 'tenacity',\n",
              " 'lemmi',\n",
              " 'overcompensate',\n",
              " 'headboard',\n",
              " 'taught',\n",
              " 'chuck',\n",
              " 'irritated',\n",
              " 'defintely',\n",
              " 'bag',\n",
              " 'core',\n",
              " 'caring',\n",
              " 'healthier',\n",
              " 'paraguay',\n",
              " 'early',\n",
              " 'gee',\n",
              " 'deficit',\n",
              " 'singapore',\n",
              " 'wo',\n",
              " 'search',\n",
              " 'surgery',\n",
              " 'riled',\n",
              " 'scholastically',\n",
              " 'cu',\n",
              " 'joy',\n",
              " 'dwindling',\n",
              " 'drastic',\n",
              " 'becomes',\n",
              " 'lying',\n",
              " 'lead',\n",
              " 'gullible',\n",
              " 'draconian',\n",
              " 'pleaser',\n",
              " 'emotional',\n",
              " 'horseback',\n",
              " 'diploma',\n",
              " 'photograph',\n",
              " 'trudging',\n",
              " 'title',\n",
              " 'dined',\n",
              " 'ego',\n",
              " 'wasted',\n",
              " 'company',\n",
              " 'readjust',\n",
              " 'sale',\n",
              " 'engaged',\n",
              " 'cousin',\n",
              " 'el',\n",
              " 'humanity',\n",
              " 'latest',\n",
              " 'legal',\n",
              " 'worst',\n",
              " 'atmosphere',\n",
              " 'ordinary',\n",
              " 'pursue',\n",
              " 'grandpa',\n",
              " 'battling',\n",
              " 'kicker',\n",
              " 'backpack',\n",
              " 'skateboard',\n",
              " 'especially',\n",
              " 'cussing',\n",
              " 'talking',\n",
              " 'sixteen',\n",
              " 'smoothe',\n",
              " 'rift',\n",
              " 'neurological',\n",
              " 'whose',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wOQoNNIYOKO",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBMsnKpiHFso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "7b25c5ac-4464-4ad8-f1ea-0096d6fe812c"
      },
      "source": [
        "windows_size = WINDOWS_SIZE\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(all_participants_mix['clean_answer']) # fit_on_texts creates the vocabulary index based on word frequency.\n",
        "#The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word \n",
        "\n",
        "tokenizer.fit_on_sequences(all_participants_mix['clean_answer']) #texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
        "\n",
        "all_participants_mix['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix['clean_answer'])\n",
        "all_participants_mix.head(15)\n",
        "\n",
        "#   why are the output as numbers when text_to_sequences is called?\n",
        "# the Tokenizer stores everything in the word_index during fit_on_texts. Then, when calling the texts_to_sequences method, only the top num_words are considered."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>personId</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>t_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>300</td>\n",
              "      <td>how are you doing today</td>\n",
              "      <td>good</td>\n",
              "      <td>[good]</td>\n",
              "      <td>[14]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>300</td>\n",
              "      <td>where are you from originally</td>\n",
              "      <td>atlanta georgia</td>\n",
              "      <td>[atlanta, georgia]</td>\n",
              "      <td>[1809, 2053]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>300</td>\n",
              "      <td>why'd you move to l_a</td>\n",
              "      <td>um my parents are from here um</td>\n",
              "      <td>[um, parent, um]</td>\n",
              "      <td>[1, 125, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>300</td>\n",
              "      <td>how do you like l_a</td>\n",
              "      <td>i love it</td>\n",
              "      <td>[love]</td>\n",
              "      <td>[64]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>300</td>\n",
              "      <td>what are some things you really like about l_a</td>\n",
              "      <td>i like the weather i like the opportunities u...</td>\n",
              "      <td>[like, weather, like, opportunity, um, yes]</td>\n",
              "      <td>[5, 132, 5, 327, 1, 41]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>300</td>\n",
              "      <td>how easy was it for you to get used to living ...</td>\n",
              "      <td>um it took a minute somewhat easy</td>\n",
              "      <td>[um, took, minute, somewhat, easy]</td>\n",
              "      <td>[1, 164, 511, 581, 85]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>300</td>\n",
              "      <td>what are some things you don't really like abo...</td>\n",
              "      <td>congestion</td>\n",
              "      <td>[congestion]</td>\n",
              "      <td>[1810]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>300</td>\n",
              "      <td>what'd you study at school</td>\n",
              "      <td>um i took up business and administration</td>\n",
              "      <td>[um, took, business, administration]</td>\n",
              "      <td>[1, 164, 181, 1246]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>300</td>\n",
              "      <td>are you still doing that</td>\n",
              "      <td>uh yeah i am here and there i'm on a break ri...</td>\n",
              "      <td>[uh, yeah, i, am, break, right, plan, going, b...</td>\n",
              "      <td>[2, 19, 3, 6, 605, 46, 813, 38, 43, 2, 429, 1247]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>300</td>\n",
              "      <td>what's your dream job</td>\n",
              "      <td>uh probably to open up my own business</td>\n",
              "      <td>[uh, probably, open, business]</td>\n",
              "      <td>[2, 27, 331, 181]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>300</td>\n",
              "      <td>do you travel a lot</td>\n",
              "      <td>no</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>300</td>\n",
              "      <td>why</td>\n",
              "      <td>um no specific reason i just don't travel a l...</td>\n",
              "      <td>[um, specific, reason, travel, lot, i, am, pre...</td>\n",
              "      <td>[1, 887, 275, 191, 18, 3, 6, 22, 1248]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>300</td>\n",
              "      <td>how often do you go back to your hometown</td>\n",
              "      <td>once a year</td>\n",
              "      <td>[year]</td>\n",
              "      <td>[23]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>300</td>\n",
              "      <td>do you consider yourself an introvert</td>\n",
              "      <td>can you be a little bit more specific</td>\n",
              "      <td>[little, bit, specific]</td>\n",
              "      <td>[29, 83, 887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>300</td>\n",
              "      <td>whatever comes to your mind</td>\n",
              "      <td>no answer</td>\n",
              "      <td>[answer]</td>\n",
              "      <td>[383]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   personId  ...                                           t_answer\n",
              "0       300  ...                                               [14]\n",
              "1       300  ...                                       [1809, 2053]\n",
              "2       300  ...                                        [1, 125, 1]\n",
              "3       300  ...                                               [64]\n",
              "4       300  ...                            [5, 132, 5, 327, 1, 41]\n",
              "5       300  ...                             [1, 164, 511, 581, 85]\n",
              "6       300  ...                                             [1810]\n",
              "7       300  ...                                [1, 164, 181, 1246]\n",
              "8       300  ...  [2, 19, 3, 6, 605, 46, 813, 38, 43, 2, 429, 1247]\n",
              "9       300  ...                                  [2, 27, 331, 181]\n",
              "10      300  ...                                                 []\n",
              "11      300  ...             [1, 887, 275, 191, 18, 3, 6, 22, 1248]\n",
              "12      300  ...                                               [23]\n",
              "13      300  ...                                      [29, 83, 887]\n",
              "14      300  ...                                              [383]\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81uq4fnuLebm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98d46cea-d9fa-403f-c4d7-654f69f56225"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "word_size = len(word_index)\n",
        "print(word_index[\"sad\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYiNu5gukIAk",
        "colab_type": "text"
      },
      "source": [
        "### Loading the train, validation and test data containing the PHQ Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXUewPnosxiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_avec_dataset_file(path,score_column):\n",
        "    ds = pd.read_csv(path, sep=',')\n",
        "    ds['level'] = pd.cut(ds[score_column], bins=[-1,0,5,10,15,25], labels=[0,1,2,3,4])  #cut function used to segregate array into bins 5 levels - 'none','mild','moderate','moderately severe', 'severe'\n",
        "    ds['PHQ8_Score'] = ds[score_column]\n",
        "    ds['cat_level'] = keras.utils.to_categorical(ds['level'], num_classes).tolist() #categorical levels \n",
        "    ds = ds[['Participant_ID', 'level', 'cat_level', 'PHQ8_Score','Gender']] \n",
        "    ds = ds.astype({\"Participant_ID\": float, \"level\": int, 'PHQ8_Score': int})\n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxYpM5l9voi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "eae70133-df1a-4a0f-aac5-a34b98d9ef55"
      },
      "source": [
        "train = load_avec_dataset_file('/content/drive/My Drive/Depression_detect/train_split_Depression_AVEC2017 (1).csv','PHQ8_Score')\n",
        "dev = load_avec_dataset_file('/content/drive/My Drive/Depression_detect/dev_split_Depression_AVEC2017.csv','PHQ8_Score')\n",
        "test = load_avec_dataset_file('/content/drive/My Drive/Depression_detect/full_test_split.csv','PHQ8_Score')\n",
        "print(\"Size: train= {}, dev= {}, test={}\".format(len(train), len(dev), len(test)))\n",
        "train.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: train= 107, dev= 35, test=47\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>level</th>\n",
              "      <th>cat_level</th>\n",
              "      <th>PHQ8_Score</th>\n",
              "      <th>Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>303.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>304.0</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>305.0</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>310.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>312.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Participant_ID  level                  cat_level  PHQ8_Score  Gender\n",
              "0           303.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       0\n",
              "1           304.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           6       0\n",
              "2           305.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           7       1\n",
              "3           310.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           4       1\n",
              "4           312.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           2       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJtt2WSUyJf2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f84b72b-2ee4-4a18-c1ec-b4ca7f4bb40a"
      },
      "source": [
        "ds_total = pd.concat([dev,train,test])\n",
        "total_phq8 = len(ds_total)\n",
        "print(\"Total size = {}\".format(total_phq8))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total size = 189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dXT735x-Gn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "7cad4c3c-4594-4b19-e50a-707bee02eef3"
      },
      "source": [
        "ds_total"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Participant_ID</th>\n",
              "      <th>level</th>\n",
              "      <th>cat_level</th>\n",
              "      <th>PHQ8_Score</th>\n",
              "      <th>Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>302.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>307.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>331.0</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>335.0</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>346.0</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>467.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>469.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>470.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>480.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>481.0</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>189 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Participant_ID  level                  cat_level  PHQ8_Score  Gender\n",
              "0            302.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           4       1\n",
              "1            307.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           4       0\n",
              "2            331.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           8       1\n",
              "3            335.0      3  [0.0, 0.0, 0.0, 1.0, 0.0]          12       0\n",
              "4            346.0      4  [0.0, 0.0, 0.0, 0.0, 1.0]          23       0\n",
              "..             ...    ...                        ...         ...     ...\n",
              "42           467.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       1\n",
              "43           469.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           3       0\n",
              "44           470.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           3       0\n",
              "45           480.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           1       1\n",
              "46           481.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           7       1\n",
              "\n",
              "[189 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_NwXFRKw__w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11fd7418-fc6a-4f6e-e4cb-6c7385e0441a"
      },
      "source": [
        "ds_total.to_csv('/content/drive/My Drive/Depression_detect/ds_total.csv', sep='\\t')\n",
        "print(\"File was created\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File was created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXjdhA9VkbqV",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the dataset and grouping them based on the 5 different level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZm8A3XG-Uu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_by_phq_level(ds):\n",
        "    none_ds = ds[ds['level']==0]\n",
        "    mild_ds = ds[ds['level']==1]\n",
        "    moderate_ds = ds[ds['level']==2]\n",
        "    moderate_severe_ds = ds[ds['level']==3]\n",
        "    severe_ds = ds[ds['level']==4]\n",
        "    return (none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzuAE0BO-mcy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b914129-6b30-4741-c182-f7c4e3fd764f"
      },
      "source": [
        "none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_total)\n",
        "print(\"Quantity per none_ds: {}, mild_ds: {}, moderate_ds {}, moderate_severe_ds: {}, severe_ds {}\".format(len(none_ds), len(mild_ds), len(moderate_ds), len(moderate_severe_ds), len(severe_ds)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quantity per none_ds: 26, mild_ds: 70, moderate_ds 47, moderate_severe_ds: 24, severe_ds 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBg3OyJKk3CG",
        "colab_type": "text"
      },
      "source": [
        "### Merging the phrases_lp (consisting of the transcripts) and ds_total consisting of PHQ Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccRbZsAG-29q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c8e786a6-f1d7-4be8-c17c-da00c4eadf07"
      },
      "source": [
        "ds_lp = ds_total.merge(all_participants_mix,left_on=ds_total.Participant_ID.astype(int), right_on=all_participants_mix.personId.astype(int))\n",
        "ds_lp.drop(ds_lp[ds_lp[\"t_answer\"].map(len) < 3].index, inplace = True)\n",
        "ds_lp.drop(['key_0','Participant_ID','cat_level','PHQ8_Score',\t'Gender',\t'personId',\t'question'],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level</th>\n",
              "      <th>answer</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>t_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>i'm fine how about yourself</td>\n",
              "      <td>[i, am, fine]</td>\n",
              "      <td>[3, 6, 266]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>i'm from los angeles california</td>\n",
              "      <td>[i, am, los, angeles, california]</td>\n",
              "      <td>[3, 6, 216, 221, 232]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>um all my family's here friends a mixture of ...</td>\n",
              "      <td>[um, family, friend, mixture, people, lot, thing]</td>\n",
              "      <td>[1, 59, 34, 2090, 13, 18, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>early childhood education</td>\n",
              "      <td>[early, childhood, education]</td>\n",
              "      <td>[463, 899, 370]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>no not right now but i would love to get back...</td>\n",
              "      <td>[right, would, love, get, back]</td>\n",
              "      <td>[46, 10, 64, 17, 43]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8697</th>\n",
              "      <td>2</td>\n",
              "      <td>i thought you weren't a therapist that's a ty...</td>\n",
              "      <td>[thought, therapist, that, typical, therapist,...</td>\n",
              "      <td>[100, 352, 20, 2032, 352, 186, 46, 9]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8698</th>\n",
              "      <td>2</td>\n",
              "      <td>um yesterday i was watching a movie with my g...</td>\n",
              "      <td>[um, yesterday, watching, movie, girlfriend, l...</td>\n",
              "      <td>[1, 369, 474, 202, 252, 883, 5293, 97]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8699</th>\n",
              "      <td>2</td>\n",
              "      <td>um very honest um sometimes brutally so but i...</td>\n",
              "      <td>[um, honest, um, sometimes, brutally, i, am, g...</td>\n",
              "      <td>[1, 388, 1, 44, 3338, 3, 6, 14, 281, 12, 34, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8700</th>\n",
              "      <td>2</td>\n",
              "      <td>um i guess well it's what i'm working on whic...</td>\n",
              "      <td>[um, guess, well, i, am, working, trying, impr...</td>\n",
              "      <td>[1, 35, 15, 3, 6, 116, 96, 964, 1405, 57, 53, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8701</th>\n",
              "      <td>2</td>\n",
              "      <td>it gets better &lt;sigh&gt; um stay in shape it get...</td>\n",
              "      <td>[get, better, sigh, um, stay, shape, get, hard...</td>\n",
              "      <td>[17, 80, 24, 1, 150, 1996, 17, 505, 683, 1031,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6722 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      level  ...                                           t_answer\n",
              "0         1  ...                                        [3, 6, 266]\n",
              "1         1  ...                              [3, 6, 216, 221, 232]\n",
              "3         1  ...                       [1, 59, 34, 2090, 13, 18, 8]\n",
              "4         1  ...                                    [463, 899, 370]\n",
              "5         1  ...                               [46, 10, 64, 17, 43]\n",
              "...     ...  ...                                                ...\n",
              "8697      2  ...              [100, 352, 20, 2032, 352, 186, 46, 9]\n",
              "8698      2  ...             [1, 369, 474, 202, 252, 883, 5293, 97]\n",
              "8699      2  ...  [1, 388, 1, 44, 3338, 3, 6, 14, 281, 12, 34, 1...\n",
              "8700      2  ...  [1, 35, 15, 3, 6, 116, 96, 964, 1405, 57, 53, ...\n",
              "8701      2  ...  [17, 80, 24, 1, 150, 1996, 17, 505, 683, 1031,...\n",
              "\n",
              "[6722 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNiNyB4uaQNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6fbc3556-4c9a-4eb7-cc82-f2e8bd91de33"
      },
      "source": [
        "ds_lp.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['key_0', 'Participant_ID', 'level', 'cat_level', 'PHQ8_Score', 'Gender',\n",
              "       'personId', 'question', 'answer', 'clean_answer', 't_answer'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AElmXBvmQcC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(ds_lp['clean_answer'], ds_lp['level'], test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuJ-MFwOkoyL",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQfvOYJJR2js",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cf62d0ed-aa27-4378-bb25-7fd98c92b31c"
      },
      "source": [
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Initalise the TfIdf vectoriser \n",
        "tvec = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)\n",
        "# Fit the training data on the model\n",
        "tvec.fit(X_train)\n",
        "\n",
        "# Transform training data into sparse matrix\n",
        "X_train_tvec = tvec.transform(X_train)\n",
        "# Transform training data into sparse matrix\n",
        "X_test_tvec = tvec.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning:\n",
            "\n",
            "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvAtjnYPhoYI",
        "colab_type": "text"
      },
      "source": [
        "## SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncN3-lCZd_Ad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "97da5452-fc19-49ba-cd4e-44fefbc975af"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
        "lr = LogisticRegression(random_state=1)\n",
        "\n",
        "tvec_score = cross_val_score(lr, X_train_tvec, y_train, cv=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpwz0bShS101",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a31f275-a125-42d7-e918-f4aff91ce480"
      },
      "source": [
        "print('Tfidf Vectorizer Score:', tvec_score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tfidf Vectorizer Score: 0.38255189659522487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C68yUJTJeB7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = baseline_model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuoUnv0ueIwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cea4e0c8-3141-4c57-c0f8-236983fa715d"
      },
      "source": [
        "accuracy_score(y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3738225086762519"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysj0Ss0BeL9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ffb59446-b389-42c5-d5b6-6c4668d36c5f"
      },
      "source": [
        "print(classification_report(y_test, predictions, digits=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.36585   0.04505   0.08021       333\n",
            "           1    0.38149   0.85857   0.52826       898\n",
            "           2    0.35762   0.25392   0.29698       638\n",
            "           3    0.33333   0.00272   0.00539       368\n",
            "           4    0.27273   0.01027   0.01980       292\n",
            "\n",
            "    accuracy                        0.37643      2529\n",
            "   macro avg    0.34220   0.23411   0.18613      2529\n",
            "weighted avg    0.35384   0.37643   0.27613      2529\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDw7RIuyhrse",
        "colab_type": "text"
      },
      "source": [
        "# GloVe + Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPawtfnFePzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(ds_lp['answer'], ds_lp['level'], test_size=0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnTS3za7kqWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f4225d8e-aec3-4393-8bf8-c37199334272"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-ziDZdwiCTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b9d37732-0498-48ed-ed13-7a1eb95ecf4e"
      },
      "source": [
        "y_pred = nb.predict(x_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred, digits=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.35746157659890926\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.22857   0.05755   0.09195       278\n",
            "           1    0.38192   0.74011   0.50385       708\n",
            "           2    0.31106   0.29563   0.30315       504\n",
            "           3    0.33333   0.07116   0.11728       267\n",
            "           4    0.33333   0.05000   0.08696       260\n",
            "\n",
            "    accuracy                        0.35746      2017\n",
            "   macro avg    0.31765   0.24289   0.22064      2017\n",
            "weighted avg    0.33039   0.35746   0.29202      2017\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3IlCGPqk0lO",
        "colab_type": "text"
      },
      "source": [
        "## GloVe + Logistric Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nttb7dkTk-rA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "ddcf0802-fdc2-4b18-f1a5-12698f7d87e2"
      },
      "source": [
        "phrases_lp = pd.read_csv('/content/drive/My Drive/Final-year-project-Phase 2/phrases_lp.csv', sep='\\t', converters={\"t_answer\": literal_eval}) \n",
        "phrases_lp.head(15)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>personId</th>\n",
              "      <th>answer</th>\n",
              "      <th>t_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>300</td>\n",
              "      <td>['good', 'atlanta', 'georgia', 'um', 'parent',...</td>\n",
              "      <td>[16, 1634, 1997, 1, 131, 1, 63, 5, 142, 5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>300</td>\n",
              "      <td>['atlanta', 'georgia', 'um', 'parent', 'um', '...</td>\n",
              "      <td>[1634, 1997, 1, 131, 1, 63, 5, 142, 5, 334]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>300</td>\n",
              "      <td>['georgia', 'um', 'parent', 'um', 'love', 'lik...</td>\n",
              "      <td>[1997, 1, 131, 1, 63, 5, 142, 5, 334, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>300</td>\n",
              "      <td>['um', 'parent', 'um', 'love', 'like', 'weathe...</td>\n",
              "      <td>[1, 131, 1, 63, 5, 142, 5, 334, 1, 39]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>300</td>\n",
              "      <td>['parent', 'um', 'love', 'like', 'weather', 'l...</td>\n",
              "      <td>[131, 1, 63, 5, 142, 5, 334, 1, 39, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>300</td>\n",
              "      <td>['um', 'love', 'like', 'weather', 'like', 'opp...</td>\n",
              "      <td>[1, 63, 5, 142, 5, 334, 1, 39, 1, 154]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>300</td>\n",
              "      <td>['love', 'like', 'weather', 'like', 'opportuni...</td>\n",
              "      <td>[63, 5, 142, 5, 334, 1, 39, 1, 154, 527]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>300</td>\n",
              "      <td>['like', 'weather', 'like', 'opportunity', 'um...</td>\n",
              "      <td>[5, 142, 5, 334, 1, 39, 1, 154, 527, 608]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>300</td>\n",
              "      <td>['weather', 'like', 'opportunity', 'um', 'yes'...</td>\n",
              "      <td>[142, 5, 334, 1, 39, 1, 154, 527, 608, 100]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>300</td>\n",
              "      <td>['like', 'opportunity', 'um', 'yes', 'um', 'to...</td>\n",
              "      <td>[5, 334, 1, 39, 1, 154, 527, 608, 100, 1998]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>300</td>\n",
              "      <td>['opportunity', 'um', 'yes', 'um', 'took', 'mi...</td>\n",
              "      <td>[334, 1, 39, 1, 154, 527, 608, 100, 1998, 20]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>300</td>\n",
              "      <td>['um', 'yes', 'um', 'took', 'minute', 'somewha...</td>\n",
              "      <td>[1, 39, 1, 154, 527, 608, 100, 1998, 20, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>300</td>\n",
              "      <td>['yes', 'um', 'took', 'minute', 'somewhat', 'e...</td>\n",
              "      <td>[39, 1, 154, 527, 608, 100, 1998, 20, 1, 154]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>300</td>\n",
              "      <td>['um', 'took', 'minute', 'somewhat', 'easy', '...</td>\n",
              "      <td>[1, 154, 527, 608, 100, 1998, 20, 1, 154, 188]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>300</td>\n",
              "      <td>['took', 'minute', 'somewhat', 'easy', 'conges...</td>\n",
              "      <td>[154, 527, 608, 100, 1998, 20, 1, 154, 188, 1376]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0  ...                                           t_answer\n",
              "0            0  ...         [16, 1634, 1997, 1, 131, 1, 63, 5, 142, 5]\n",
              "1            1  ...        [1634, 1997, 1, 131, 1, 63, 5, 142, 5, 334]\n",
              "2            2  ...           [1997, 1, 131, 1, 63, 5, 142, 5, 334, 1]\n",
              "3            3  ...             [1, 131, 1, 63, 5, 142, 5, 334, 1, 39]\n",
              "4            4  ...             [131, 1, 63, 5, 142, 5, 334, 1, 39, 1]\n",
              "5            5  ...             [1, 63, 5, 142, 5, 334, 1, 39, 1, 154]\n",
              "6            6  ...           [63, 5, 142, 5, 334, 1, 39, 1, 154, 527]\n",
              "7            7  ...          [5, 142, 5, 334, 1, 39, 1, 154, 527, 608]\n",
              "8            8  ...        [142, 5, 334, 1, 39, 1, 154, 527, 608, 100]\n",
              "9            9  ...       [5, 334, 1, 39, 1, 154, 527, 608, 100, 1998]\n",
              "10          10  ...      [334, 1, 39, 1, 154, 527, 608, 100, 1998, 20]\n",
              "11          11  ...        [1, 39, 1, 154, 527, 608, 100, 1998, 20, 1]\n",
              "12          12  ...      [39, 1, 154, 527, 608, 100, 1998, 20, 1, 154]\n",
              "13          13  ...     [1, 154, 527, 608, 100, 1998, 20, 1, 154, 188]\n",
              "14          14  ...  [154, 527, 608, 100, 1998, 20, 1, 154, 188, 1376]\n",
              "\n",
              "[15 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxp3ijh8lNV_",
        "colab_type": "text"
      },
      "source": [
        "## Random Sampling of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzR7nhy8lcgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_none_ds = ds_total[ds_total['level']==0]\n",
        "b_mild_ds = ds_total[ds_total['level']==1].sample(26)\n",
        "b_moderate_ds = ds_total[ds_total['level']==2].sample(26)\n",
        "b_moderate_severe_ds = ds_total[ds_total['level']==3]\n",
        "b_severe_ds = ds_total[ds_total['level']==4]\n",
        "\n",
        "ds_total_b = pd.concat([b_none_ds, b_mild_ds, b_moderate_ds, b_moderate_severe_ds, b_severe_ds])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zv4dTbBk7up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_lp = pd.merge(ds_total, phrases_lp,left_on='Participant_ID', right_on='personId')\n",
        "ds_lp.drop(ds_lp[ds_lp[\"t_answer\"].map(len) < 10].index, inplace = True)\n",
        "ds_lp_b = pd.merge(ds_total_b, phrases_lp,left_on='Participant_ID', right_on='personId')\n",
        "ds_lp_b.drop(ds_lp_b[ds_lp_b[\"t_answer\"].map(len) < 10].index, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olykDkeMoeXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(ds_lp['answer'], ds_lp['level'], test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHzMHn_KltnK",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Glove Embeddings into a file and putting each vector into an np array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3c7vax-ljZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4b0b178-f9ce-44d3-fbb4-970343f561ea"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/drive/My Drive/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "for line in tqdm(f):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "       coefs = np.asarray(values[1:], dtype='float32')\n",
        "       embeddings_index[word] = coefs\n",
        "    except ValueError:\n",
        "       pass\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:12, 32573.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKJ9CUqUl0Op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d0a28c6-d21a-4d1a-ad66-4fdaa685fc8d"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "def sent2vec(s):\n",
        "    words = str(s).lower()\n",
        "    words = word_tokenize(words)\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    M = []\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(embeddings_index[w])\n",
        "        except:\n",
        "            continue\n",
        "    M = np.array(M)\n",
        "    v = M.sum(axis=0)\n",
        "    if type(v) != np.ndarray:\n",
        "        return np.zeros(100)\n",
        "    return v / np.sqrt((v ** 2).sum())\n",
        "\n",
        "xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\n",
        "xtest_glove = [sent2vec(x) for x in tqdm(X_test)]\n",
        "\n",
        "print('Checkpoint2 -Normalized Vector for Sentences are created')\n",
        "\n",
        "xtrain_glove = np.array(xtrain_glove)\n",
        "xtest_glove = np.array(xtest_glove)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/112909 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 343/112909 [00:00<00:32, 3426.36it/s]\u001b[A\n",
            "  1%|          | 635/112909 [00:00<00:34, 3254.00it/s]\u001b[A\n",
            "  1%|          | 957/112909 [00:00<00:34, 3243.33it/s]\u001b[A\n",
            "  1%|          | 1301/112909 [00:00<00:33, 3298.36it/s]\u001b[A\n",
            "  1%|▏         | 1651/112909 [00:00<00:33, 3354.95it/s]\u001b[A\n",
            "  2%|▏         | 1972/112909 [00:00<00:33, 3308.77it/s]\u001b[A\n",
            "  2%|▏         | 2322/112909 [00:00<00:32, 3363.45it/s]\u001b[A\n",
            "  2%|▏         | 2669/112909 [00:00<00:32, 3392.49it/s]\u001b[A\n",
            "  3%|▎         | 3014/112909 [00:00<00:32, 3408.03it/s]\u001b[A\n",
            "  3%|▎         | 3362/112909 [00:01<00:31, 3427.79it/s]\u001b[A\n",
            "  3%|▎         | 3707/112909 [00:01<00:31, 3433.33it/s]\u001b[A\n",
            "  4%|▎         | 4060/112909 [00:01<00:31, 3460.98it/s]\u001b[A\n",
            "  4%|▍         | 4406/112909 [00:01<00:31, 3460.14it/s]\u001b[A\n",
            "  4%|▍         | 4749/112909 [00:01<00:31, 3442.01it/s]\u001b[A\n",
            "  5%|▍         | 5092/112909 [00:01<00:31, 3419.87it/s]\u001b[A\n",
            "  5%|▍         | 5433/112909 [00:01<00:32, 3321.17it/s]\u001b[A\n",
            "  5%|▌         | 5781/112909 [00:01<00:31, 3367.14it/s]\u001b[A\n",
            "  5%|▌         | 6132/112909 [00:01<00:31, 3407.32it/s]\u001b[A\n",
            "  6%|▌         | 6482/112909 [00:01<00:30, 3434.52it/s]\u001b[A\n",
            "  6%|▌         | 6826/112909 [00:02<00:31, 3382.55it/s]\u001b[A\n",
            "  6%|▋         | 7178/112909 [00:02<00:30, 3420.76it/s]\u001b[A\n",
            "  7%|▋         | 7530/112909 [00:02<00:30, 3447.12it/s]\u001b[A\n",
            "  7%|▋         | 7875/112909 [00:02<00:30, 3444.41it/s]\u001b[A\n",
            "  7%|▋         | 8227/112909 [00:02<00:30, 3464.99it/s]\u001b[A\n",
            "  8%|▊         | 8574/112909 [00:02<00:30, 3464.03it/s]\u001b[A\n",
            "  8%|▊         | 8921/112909 [00:02<00:30, 3407.65it/s]\u001b[A\n",
            "  8%|▊         | 9272/112909 [00:02<00:30, 3435.26it/s]\u001b[A\n",
            "  9%|▊         | 9616/112909 [00:02<00:30, 3426.46it/s]\u001b[A\n",
            "  9%|▉         | 9959/112909 [00:02<00:30, 3425.78it/s]\u001b[A\n",
            "  9%|▉         | 10315/112909 [00:03<00:29, 3462.79it/s]\u001b[A\n",
            "  9%|▉         | 10663/112909 [00:03<00:29, 3465.54it/s]\u001b[A\n",
            " 10%|▉         | 11010/112909 [00:03<00:29, 3442.32it/s]\u001b[A\n",
            " 10%|█         | 11355/112909 [00:03<00:29, 3443.16it/s]\u001b[A\n",
            " 10%|█         | 11707/112909 [00:03<00:29, 3465.79it/s]\u001b[A\n",
            " 11%|█         | 12055/112909 [00:03<00:29, 3468.12it/s]\u001b[A\n",
            " 11%|█         | 12402/112909 [00:03<00:29, 3385.25it/s]\u001b[A\n",
            " 11%|█▏        | 12747/112909 [00:03<00:29, 3404.35it/s]\u001b[A\n",
            " 12%|█▏        | 13097/112909 [00:03<00:29, 3431.67it/s]\u001b[A\n",
            " 12%|█▏        | 13445/112909 [00:03<00:28, 3443.71it/s]\u001b[A\n",
            " 12%|█▏        | 13798/112909 [00:04<00:28, 3468.19it/s]\u001b[A\n",
            " 13%|█▎        | 14146/112909 [00:04<00:28, 3419.69it/s]\u001b[A\n",
            " 13%|█▎        | 14494/112909 [00:04<00:28, 3436.69it/s]\u001b[A\n",
            " 13%|█▎        | 14842/112909 [00:04<00:28, 3449.02it/s]\u001b[A\n",
            " 13%|█▎        | 15194/112909 [00:04<00:28, 3469.21it/s]\u001b[A\n",
            " 14%|█▍        | 15544/112909 [00:04<00:28, 3476.03it/s]\u001b[A\n",
            " 14%|█▍        | 15892/112909 [00:04<00:28, 3396.71it/s]\u001b[A\n",
            " 14%|█▍        | 16234/112909 [00:04<00:28, 3403.41it/s]\u001b[A\n",
            " 15%|█▍        | 16584/112909 [00:04<00:28, 3430.94it/s]\u001b[A\n",
            " 15%|█▍        | 16928/112909 [00:04<00:27, 3428.01it/s]\u001b[A\n",
            " 15%|█▌        | 17276/112909 [00:05<00:27, 3441.67it/s]\u001b[A\n",
            " 16%|█▌        | 17626/112909 [00:05<00:27, 3457.32it/s]\u001b[A\n",
            " 16%|█▌        | 17982/112909 [00:05<00:27, 3486.16it/s]\u001b[A\n",
            " 16%|█▌        | 18338/112909 [00:05<00:26, 3505.79it/s]\u001b[A\n",
            " 17%|█▋        | 18689/112909 [00:05<00:26, 3495.37it/s]\u001b[A\n",
            " 17%|█▋        | 19046/112909 [00:05<00:26, 3516.31it/s]\u001b[A\n",
            " 17%|█▋        | 19398/112909 [00:05<00:27, 3431.03it/s]\u001b[A\n",
            " 17%|█▋        | 19742/112909 [00:05<00:27, 3428.73it/s]\u001b[A\n",
            " 18%|█▊        | 20086/112909 [00:05<00:27, 3407.51it/s]\u001b[A\n",
            " 18%|█▊        | 20428/112909 [00:05<00:27, 3410.08it/s]\u001b[A\n",
            " 18%|█▊        | 20770/112909 [00:06<00:27, 3309.47it/s]\u001b[A\n",
            " 19%|█▊        | 21102/112909 [00:06<00:27, 3302.29it/s]\u001b[A\n",
            " 19%|█▉        | 21450/112909 [00:06<00:27, 3351.99it/s]\u001b[A\n",
            " 19%|█▉        | 21794/112909 [00:06<00:26, 3377.87it/s]\u001b[A\n",
            " 20%|█▉        | 22139/112909 [00:06<00:26, 3397.21it/s]\u001b[A\n",
            " 20%|█▉        | 22480/112909 [00:06<00:27, 3322.57it/s]\u001b[A\n",
            " 20%|██        | 22813/112909 [00:06<00:27, 3218.61it/s]\u001b[A\n",
            " 21%|██        | 23162/112909 [00:06<00:27, 3293.59it/s]\u001b[A\n",
            " 21%|██        | 23508/112909 [00:06<00:26, 3339.57it/s]\u001b[A\n",
            " 21%|██        | 23851/112909 [00:06<00:26, 3365.93it/s]\u001b[A\n",
            " 21%|██▏       | 24189/112909 [00:07<00:26, 3326.20it/s]\u001b[A\n",
            " 22%|██▏       | 24540/112909 [00:07<00:26, 3377.82it/s]\u001b[A\n",
            " 22%|██▏       | 24890/112909 [00:07<00:25, 3412.50it/s]\u001b[A\n",
            " 22%|██▏       | 25236/112909 [00:07<00:25, 3425.62it/s]\u001b[A\n",
            " 23%|██▎       | 25589/112909 [00:07<00:25, 3455.32it/s]\u001b[A\n",
            " 23%|██▎       | 25935/112909 [00:07<00:25, 3451.29it/s]\u001b[A\n",
            " 23%|██▎       | 26281/112909 [00:07<00:25, 3372.93it/s]\u001b[A\n",
            " 24%|██▎       | 26628/112909 [00:07<00:25, 3401.23it/s]\u001b[A\n",
            " 24%|██▍       | 26969/112909 [00:07<00:25, 3305.64it/s]\u001b[A\n",
            " 24%|██▍       | 27301/112909 [00:08<00:25, 3306.09it/s]\u001b[A\n",
            " 24%|██▍       | 27645/112909 [00:08<00:25, 3343.07it/s]\u001b[A\n",
            " 25%|██▍       | 27983/112909 [00:08<00:25, 3351.83it/s]\u001b[A\n",
            " 25%|██▌       | 28321/112909 [00:08<00:25, 3359.08it/s]\u001b[A\n",
            " 25%|██▌       | 28662/112909 [00:08<00:24, 3373.85it/s]\u001b[A\n",
            " 26%|██▌       | 29000/112909 [00:08<00:24, 3365.22it/s]\u001b[A\n",
            " 26%|██▌       | 29345/112909 [00:08<00:24, 3388.59it/s]\u001b[A\n",
            " 26%|██▋       | 29685/112909 [00:08<00:24, 3336.46it/s]\u001b[A\n",
            " 27%|██▋       | 30024/112909 [00:08<00:24, 3350.26it/s]\u001b[A\n",
            " 27%|██▋       | 30365/112909 [00:08<00:24, 3366.32it/s]\u001b[A\n",
            " 27%|██▋       | 30718/112909 [00:09<00:24, 3413.54it/s]\u001b[A\n",
            " 28%|██▊       | 31067/112909 [00:09<00:23, 3435.32it/s]\u001b[A\n",
            " 28%|██▊       | 31416/112909 [00:09<00:23, 3450.57it/s]\u001b[A\n",
            " 28%|██▊       | 31769/112909 [00:09<00:23, 3472.01it/s]\u001b[A\n",
            " 28%|██▊       | 32119/112909 [00:09<00:23, 3480.11it/s]\u001b[A\n",
            " 29%|██▉       | 32469/112909 [00:09<00:23, 3484.48it/s]\u001b[A\n",
            " 29%|██▉       | 32818/112909 [00:09<00:23, 3432.13it/s]\u001b[A\n",
            " 29%|██▉       | 33162/112909 [00:09<00:23, 3370.20it/s]\u001b[A\n",
            " 30%|██▉       | 33508/112909 [00:09<00:23, 3395.37it/s]\u001b[A\n",
            " 30%|██▉       | 33848/112909 [00:09<00:23, 3369.01it/s]\u001b[A\n",
            " 30%|███       | 34202/112909 [00:10<00:23, 3418.29it/s]\u001b[A\n",
            " 31%|███       | 34555/112909 [00:10<00:22, 3449.13it/s]\u001b[A\n",
            " 31%|███       | 34901/112909 [00:10<00:22, 3452.21it/s]\u001b[A\n",
            " 31%|███       | 35254/112909 [00:10<00:22, 3473.83it/s]\u001b[A\n",
            " 32%|███▏      | 35608/112909 [00:10<00:22, 3491.30it/s]\u001b[A\n",
            " 32%|███▏      | 35961/112909 [00:10<00:21, 3499.56it/s]\u001b[A\n",
            " 32%|███▏      | 36312/112909 [00:10<00:21, 3493.26it/s]\u001b[A\n",
            " 32%|███▏      | 36662/112909 [00:10<00:22, 3410.59it/s]\u001b[A\n",
            " 33%|███▎      | 37004/112909 [00:10<00:22, 3400.88it/s]\u001b[A\n",
            " 33%|███▎      | 37345/112909 [00:10<00:22, 3402.85it/s]\u001b[A\n",
            " 33%|███▎      | 37686/112909 [00:11<00:22, 3402.52it/s]\u001b[A\n",
            " 34%|███▎      | 38034/112909 [00:11<00:21, 3425.17it/s]\u001b[A\n",
            " 34%|███▍      | 38377/112909 [00:11<00:21, 3422.77it/s]\u001b[A\n",
            " 34%|███▍      | 38720/112909 [00:11<00:21, 3421.27it/s]\u001b[A\n",
            " 35%|███▍      | 39063/112909 [00:11<00:21, 3419.78it/s]\u001b[A\n",
            " 35%|███▍      | 39406/112909 [00:11<00:21, 3400.74it/s]\u001b[A\n",
            " 35%|███▌      | 39747/112909 [00:11<00:21, 3390.76it/s]\u001b[A\n",
            " 36%|███▌      | 40087/112909 [00:11<00:21, 3324.14it/s]\u001b[A\n",
            " 36%|███▌      | 40426/112909 [00:11<00:21, 3342.14it/s]\u001b[A\n",
            " 36%|███▌      | 40774/112909 [00:11<00:21, 3382.01it/s]\u001b[A\n",
            " 36%|███▋      | 41113/112909 [00:12<00:21, 3377.47it/s]\u001b[A\n",
            " 37%|███▋      | 41452/112909 [00:12<00:21, 3378.49it/s]\u001b[A\n",
            " 37%|███▋      | 41790/112909 [00:12<00:21, 3359.93it/s]\u001b[A\n",
            " 37%|███▋      | 42142/112909 [00:12<00:20, 3404.29it/s]\u001b[A\n",
            " 38%|███▊      | 42490/112909 [00:12<00:20, 3426.12it/s]\u001b[A\n",
            " 38%|███▊      | 42833/112909 [00:12<00:20, 3413.00it/s]\u001b[A\n",
            " 38%|███▊      | 43179/112909 [00:12<00:20, 3426.59it/s]\u001b[A\n",
            " 39%|███▊      | 43522/112909 [00:12<00:20, 3349.38it/s]\u001b[A\n",
            " 39%|███▉      | 43874/112909 [00:12<00:20, 3396.88it/s]\u001b[A\n",
            " 39%|███▉      | 44226/112909 [00:12<00:20, 3431.98it/s]\u001b[A\n",
            " 39%|███▉      | 44581/112909 [00:13<00:19, 3464.32it/s]\u001b[A\n",
            " 40%|███▉      | 44930/112909 [00:13<00:19, 3469.89it/s]\u001b[A\n",
            " 40%|████      | 45278/112909 [00:13<00:19, 3468.82it/s]\u001b[A\n",
            " 40%|████      | 45633/112909 [00:13<00:19, 3490.61it/s]\u001b[A\n",
            " 41%|████      | 45983/112909 [00:13<00:19, 3484.99it/s]\u001b[A\n",
            " 41%|████      | 46332/112909 [00:13<00:19, 3476.24it/s]\u001b[A\n",
            " 41%|████▏     | 46680/112909 [00:13<00:19, 3469.98it/s]\u001b[A\n",
            " 42%|████▏     | 47028/112909 [00:13<00:19, 3364.97it/s]\u001b[A\n",
            " 42%|████▏     | 47374/112909 [00:13<00:19, 3390.64it/s]\u001b[A\n",
            " 42%|████▏     | 47726/112909 [00:13<00:19, 3425.83it/s]\u001b[A\n",
            " 43%|████▎     | 48070/112909 [00:14<00:18, 3422.27it/s]\u001b[A\n",
            " 43%|████▎     | 48422/112909 [00:14<00:18, 3449.32it/s]\u001b[A\n",
            " 43%|████▎     | 48775/112909 [00:14<00:18, 3470.85it/s]\u001b[A\n",
            " 44%|████▎     | 49123/112909 [00:14<00:18, 3415.31it/s]\u001b[A\n",
            " 44%|████▍     | 49471/112909 [00:14<00:18, 3432.72it/s]\u001b[A\n",
            " 44%|████▍     | 49815/112909 [00:14<00:18, 3413.65it/s]\u001b[A\n",
            " 44%|████▍     | 50164/112909 [00:14<00:18, 3433.69it/s]\u001b[A\n",
            " 45%|████▍     | 50508/112909 [00:14<00:18, 3371.70it/s]\u001b[A\n",
            " 45%|████▌     | 50846/112909 [00:14<00:18, 3278.95it/s]\u001b[A\n",
            " 45%|████▌     | 51198/112909 [00:15<00:18, 3345.99it/s]\u001b[A\n",
            " 46%|████▌     | 51546/112909 [00:15<00:18, 3385.05it/s]\u001b[A\n",
            " 46%|████▌     | 51898/112909 [00:15<00:17, 3422.65it/s]\u001b[A\n",
            " 46%|████▋     | 52245/112909 [00:15<00:17, 3435.03it/s]\u001b[A\n",
            " 47%|████▋     | 52589/112909 [00:15<00:17, 3435.35it/s]\u001b[A\n",
            " 47%|████▋     | 52939/112909 [00:15<00:17, 3452.56it/s]\u001b[A\n",
            " 47%|████▋     | 53285/112909 [00:15<00:17, 3436.58it/s]\u001b[A\n",
            " 48%|████▊     | 53638/112909 [00:15<00:17, 3462.32it/s]\u001b[A\n",
            " 48%|████▊     | 53985/112909 [00:15<00:17, 3343.91it/s]\u001b[A\n",
            " 48%|████▊     | 54321/112909 [00:15<00:17, 3299.07it/s]\u001b[A\n",
            " 48%|████▊     | 54669/112909 [00:16<00:17, 3349.32it/s]\u001b[A\n",
            " 49%|████▊     | 55005/112909 [00:16<00:17, 3347.70it/s]\u001b[A\n",
            " 49%|████▉     | 55359/112909 [00:16<00:16, 3402.22it/s]\u001b[A\n",
            " 49%|████▉     | 55703/112909 [00:16<00:16, 3411.20it/s]\u001b[A\n",
            " 50%|████▉     | 56047/112909 [00:16<00:16, 3418.13it/s]\u001b[A\n",
            " 50%|████▉     | 56399/112909 [00:16<00:16, 3445.61it/s]\u001b[A\n",
            " 50%|█████     | 56747/112909 [00:16<00:16, 3455.00it/s]\u001b[A\n",
            " 51%|█████     | 57093/112909 [00:16<00:16, 3420.08it/s]\u001b[A\n",
            " 51%|█████     | 57436/112909 [00:16<00:20, 2667.92it/s]\u001b[A\n",
            " 51%|█████     | 57729/112909 [00:17<00:23, 2368.26it/s]\u001b[A\n",
            " 51%|█████▏    | 57991/112909 [00:17<00:24, 2212.89it/s]\u001b[A\n",
            " 52%|█████▏    | 58232/112909 [00:17<00:25, 2129.58it/s]\u001b[A\n",
            " 52%|█████▏    | 58459/112909 [00:17<00:26, 2091.90it/s]\u001b[A\n",
            " 52%|█████▏    | 58679/112909 [00:17<00:26, 2059.83it/s]\u001b[A\n",
            " 52%|█████▏    | 58893/112909 [00:17<00:26, 2010.14it/s]\u001b[A\n",
            " 52%|█████▏    | 59100/112909 [00:17<00:28, 1880.69it/s]\u001b[A\n",
            " 53%|█████▎    | 59294/112909 [00:17<00:28, 1897.53it/s]\u001b[A\n",
            " 53%|█████▎    | 59490/112909 [00:18<00:27, 1915.71it/s]\u001b[A\n",
            " 53%|█████▎    | 59690/112909 [00:18<00:27, 1938.48it/s]\u001b[A\n",
            " 53%|█████▎    | 59886/112909 [00:18<00:27, 1939.97it/s]\u001b[A\n",
            " 53%|█████▎    | 60082/112909 [00:18<00:27, 1936.99it/s]\u001b[A\n",
            " 53%|█████▎    | 60277/112909 [00:18<00:27, 1935.25it/s]\u001b[A\n",
            " 54%|█████▎    | 60474/112909 [00:18<00:26, 1943.77it/s]\u001b[A\n",
            " 54%|█████▎    | 60669/112909 [00:18<00:27, 1918.16it/s]\u001b[A\n",
            " 54%|█████▍    | 60862/112909 [00:18<00:28, 1822.74it/s]\u001b[A\n",
            " 54%|█████▍    | 61046/112909 [00:18<00:30, 1677.20it/s]\u001b[A\n",
            " 54%|█████▍    | 61240/112909 [00:18<00:29, 1748.24it/s]\u001b[A\n",
            " 54%|█████▍    | 61436/112909 [00:19<00:28, 1804.81it/s]\u001b[A\n",
            " 55%|█████▍    | 61629/112909 [00:19<00:27, 1840.60it/s]\u001b[A\n",
            " 55%|█████▍    | 61816/112909 [00:19<00:28, 1819.18it/s]\u001b[A\n",
            " 55%|█████▍    | 62003/112909 [00:19<00:27, 1833.26it/s]\u001b[A\n",
            " 55%|█████▌    | 62197/112909 [00:19<00:27, 1862.04it/s]\u001b[A\n",
            " 55%|█████▌    | 62386/112909 [00:19<00:27, 1868.76it/s]\u001b[A\n",
            " 55%|█████▌    | 62579/112909 [00:19<00:26, 1885.53it/s]\u001b[A\n",
            " 56%|█████▌    | 62770/112909 [00:19<00:26, 1892.23it/s]\u001b[A\n",
            " 56%|█████▌    | 62960/112909 [00:19<00:27, 1842.00it/s]\u001b[A\n",
            " 56%|█████▌    | 63154/112909 [00:19<00:26, 1868.20it/s]\u001b[A\n",
            " 56%|█████▌    | 63347/112909 [00:20<00:26, 1883.73it/s]\u001b[A\n",
            " 56%|█████▋    | 63539/112909 [00:20<00:26, 1893.91it/s]\u001b[A\n",
            " 56%|█████▋    | 63733/112909 [00:20<00:25, 1906.30it/s]\u001b[A\n",
            " 57%|█████▋    | 63926/112909 [00:20<00:25, 1912.98it/s]\u001b[A\n",
            " 57%|█████▋    | 64119/112909 [00:20<00:25, 1916.49it/s]\u001b[A\n",
            " 57%|█████▋    | 64314/112909 [00:20<00:25, 1925.47it/s]\u001b[A\n",
            " 57%|█████▋    | 64507/112909 [00:20<00:25, 1925.05it/s]\u001b[A\n",
            " 57%|█████▋    | 64700/112909 [00:20<00:25, 1887.70it/s]\u001b[A\n",
            " 57%|█████▋    | 64889/112909 [00:20<00:26, 1803.98it/s]\u001b[A\n",
            " 58%|█████▊    | 65071/112909 [00:21<00:27, 1751.30it/s]\u001b[A\n",
            " 58%|█████▊    | 65248/112909 [00:21<00:27, 1736.60it/s]\u001b[A\n",
            " 58%|█████▊    | 65438/112909 [00:21<00:26, 1780.45it/s]\u001b[A\n",
            " 58%|█████▊    | 65631/112909 [00:21<00:25, 1822.55it/s]\u001b[A\n",
            " 58%|█████▊    | 65823/112909 [00:21<00:25, 1849.39it/s]\u001b[A\n",
            " 58%|█████▊    | 66009/112909 [00:21<00:25, 1810.20it/s]\u001b[A\n",
            " 59%|█████▊    | 66197/112909 [00:21<00:25, 1828.42it/s]\u001b[A\n",
            " 59%|█████▉    | 66390/112909 [00:21<00:25, 1856.06it/s]\u001b[A\n",
            " 59%|█████▉    | 66580/112909 [00:21<00:24, 1866.76it/s]\u001b[A\n",
            " 59%|█████▉    | 66768/112909 [00:21<00:25, 1777.70it/s]\u001b[A\n",
            " 59%|█████▉    | 66947/112909 [00:22<00:25, 1778.60it/s]\u001b[A\n",
            " 59%|█████▉    | 67141/112909 [00:22<00:25, 1823.40it/s]\u001b[A\n",
            " 60%|█████▉    | 67335/112909 [00:22<00:24, 1855.30it/s]\u001b[A\n",
            " 60%|█████▉    | 67527/112909 [00:22<00:24, 1874.18it/s]\u001b[A\n",
            " 60%|█████▉    | 67719/112909 [00:22<00:23, 1885.81it/s]\u001b[A\n",
            " 60%|██████    | 67909/112909 [00:22<00:23, 1887.70it/s]\u001b[A\n",
            " 60%|██████    | 68100/112909 [00:22<00:23, 1894.17it/s]\u001b[A\n",
            " 60%|██████    | 68290/112909 [00:22<00:23, 1895.30it/s]\u001b[A\n",
            " 61%|██████    | 68480/112909 [00:22<00:23, 1892.48it/s]\u001b[A\n",
            " 61%|██████    | 68670/112909 [00:22<00:24, 1816.32it/s]\u001b[A\n",
            " 61%|██████    | 68867/112909 [00:23<00:23, 1857.56it/s]\u001b[A\n",
            " 61%|██████    | 69058/112909 [00:23<00:23, 1872.67it/s]\u001b[A\n",
            " 61%|██████▏   | 69254/112909 [00:23<00:23, 1896.52it/s]\u001b[A\n",
            " 62%|██████▏   | 69445/112909 [00:23<00:23, 1885.74it/s]\u001b[A\n",
            " 62%|██████▏   | 69639/112909 [00:23<00:22, 1900.38it/s]\u001b[A\n",
            " 62%|██████▏   | 69830/112909 [00:23<00:22, 1897.50it/s]\u001b[A\n",
            " 62%|██████▏   | 70020/112909 [00:23<00:22, 1892.09it/s]\u001b[A\n",
            " 62%|██████▏   | 70210/112909 [00:23<00:22, 1889.51it/s]\u001b[A\n",
            " 62%|██████▏   | 70406/112909 [00:23<00:22, 1907.46it/s]\u001b[A\n",
            " 63%|██████▎   | 70597/112909 [00:24<00:22, 1842.30it/s]\u001b[A\n",
            " 63%|██████▎   | 70793/112909 [00:24<00:22, 1874.68it/s]\u001b[A\n",
            " 63%|██████▎   | 70990/112909 [00:24<00:22, 1899.78it/s]\u001b[A\n",
            " 63%|██████▎   | 71187/112909 [00:24<00:21, 1919.36it/s]\u001b[A\n",
            " 63%|██████▎   | 71382/112909 [00:24<00:21, 1925.98it/s]\u001b[A\n",
            " 63%|██████▎   | 71575/112909 [00:24<00:21, 1916.54it/s]\u001b[A\n",
            " 64%|██████▎   | 71798/112909 [00:24<00:20, 2000.18it/s]\u001b[A\n",
            " 64%|██████▍   | 72148/112909 [00:24<00:17, 2294.54it/s]\u001b[A\n",
            " 64%|██████▍   | 72483/112909 [00:24<00:15, 2532.47it/s]\u001b[A\n",
            " 64%|██████▍   | 72804/112909 [00:24<00:14, 2702.52it/s]\u001b[A\n",
            " 65%|██████▍   | 73120/112909 [00:25<00:14, 2823.40it/s]\u001b[A\n",
            " 65%|██████▌   | 73469/112909 [00:25<00:13, 2994.56it/s]\u001b[A\n",
            " 65%|██████▌   | 73827/112909 [00:25<00:12, 3149.02it/s]\u001b[A\n",
            " 66%|██████▌   | 74176/112909 [00:25<00:11, 3242.70it/s]\u001b[A\n",
            " 66%|██████▌   | 74520/112909 [00:25<00:11, 3299.40it/s]\u001b[A\n",
            " 66%|██████▋   | 74870/112909 [00:25<00:11, 3355.26it/s]\u001b[A\n",
            " 67%|██████▋   | 75210/112909 [00:25<00:11, 3353.98it/s]\u001b[A\n",
            " 67%|██████▋   | 75549/112909 [00:25<00:11, 3332.76it/s]\u001b[A\n",
            " 67%|██████▋   | 75885/112909 [00:25<00:11, 3336.86it/s]\u001b[A\n",
            " 68%|██████▊   | 76233/112909 [00:25<00:10, 3378.16it/s]\u001b[A\n",
            " 68%|██████▊   | 76572/112909 [00:26<00:10, 3323.77it/s]\u001b[A\n",
            " 68%|██████▊   | 76927/112909 [00:26<00:10, 3387.16it/s]\u001b[A\n",
            " 68%|██████▊   | 77273/112909 [00:26<00:10, 3406.67it/s]\u001b[A\n",
            " 69%|██████▊   | 77620/112909 [00:26<00:10, 3425.26it/s]\u001b[A\n",
            " 69%|██████▉   | 77964/112909 [00:26<00:10, 3418.89it/s]\u001b[A\n",
            " 69%|██████▉   | 78311/112909 [00:26<00:10, 3432.13it/s]\u001b[A\n",
            " 70%|██████▉   | 78655/112909 [00:26<00:09, 3430.24it/s]\u001b[A\n",
            " 70%|██████▉   | 79005/112909 [00:26<00:09, 3448.82it/s]\u001b[A\n",
            " 70%|███████   | 79351/112909 [00:26<00:09, 3439.90it/s]\u001b[A\n",
            " 71%|███████   | 79701/112909 [00:26<00:09, 3456.44it/s]\u001b[A\n",
            " 71%|███████   | 80047/112909 [00:27<00:09, 3399.56it/s]\u001b[A\n",
            " 71%|███████   | 80394/112909 [00:27<00:09, 3419.98it/s]\u001b[A\n",
            " 72%|███████▏  | 80742/112909 [00:27<00:09, 3435.70it/s]\u001b[A\n",
            " 72%|███████▏  | 81095/112909 [00:27<00:09, 3462.77it/s]\u001b[A\n",
            " 72%|███████▏  | 81442/112909 [00:27<00:09, 3452.53it/s]\u001b[A\n",
            " 72%|███████▏  | 81790/112909 [00:27<00:08, 3458.79it/s]\u001b[A\n",
            " 73%|███████▎  | 82136/112909 [00:27<00:09, 3165.58it/s]\u001b[A\n",
            " 73%|███████▎  | 82489/112909 [00:27<00:09, 3266.36it/s]\u001b[A\n",
            " 73%|███████▎  | 82827/112909 [00:27<00:09, 3297.17it/s]\u001b[A\n",
            " 74%|███████▎  | 83173/112909 [00:27<00:08, 3344.10it/s]\u001b[A\n",
            " 74%|███████▍  | 83510/112909 [00:28<00:08, 3315.19it/s]\u001b[A\n",
            " 74%|███████▍  | 83859/112909 [00:28<00:08, 3364.78it/s]\u001b[A\n",
            " 75%|███████▍  | 84208/112909 [00:28<00:08, 3400.02it/s]\u001b[A\n",
            " 75%|███████▍  | 84550/112909 [00:28<00:08, 3395.58it/s]\u001b[A\n",
            " 75%|███████▌  | 84897/112909 [00:28<00:08, 3416.01it/s]\u001b[A\n",
            " 76%|███████▌  | 85253/112909 [00:28<00:07, 3457.82it/s]\u001b[A\n",
            " 76%|███████▌  | 85600/112909 [00:28<00:07, 3450.12it/s]\u001b[A\n",
            " 76%|███████▌  | 85948/112909 [00:28<00:07, 3457.16it/s]\u001b[A\n",
            " 76%|███████▋  | 86294/112909 [00:28<00:07, 3406.01it/s]\u001b[A\n",
            " 77%|███████▋  | 86635/112909 [00:28<00:07, 3404.92it/s]\u001b[A\n",
            " 77%|███████▋  | 86976/112909 [00:29<00:07, 3351.88it/s]\u001b[A\n",
            " 77%|███████▋  | 87323/112909 [00:29<00:07, 3385.22it/s]\u001b[A\n",
            " 78%|███████▊  | 87665/112909 [00:29<00:07, 3393.79it/s]\u001b[A\n",
            " 78%|███████▊  | 88010/112909 [00:29<00:07, 3408.53it/s]\u001b[A\n",
            " 78%|███████▊  | 88358/112909 [00:29<00:07, 3427.62it/s]\u001b[A\n",
            " 79%|███████▊  | 88701/112909 [00:29<00:07, 3387.31it/s]\u001b[A\n",
            " 79%|███████▉  | 89042/112909 [00:29<00:07, 3393.21it/s]\u001b[A\n",
            " 79%|███████▉  | 89385/112909 [00:29<00:06, 3403.00it/s]\u001b[A\n",
            " 79%|███████▉  | 89734/112909 [00:29<00:06, 3426.50it/s]\u001b[A\n",
            " 80%|███████▉  | 90083/112909 [00:29<00:06, 3443.35it/s]\u001b[A\n",
            " 80%|████████  | 90428/112909 [00:30<00:06, 3370.21it/s]\u001b[A\n",
            " 80%|████████  | 90766/112909 [00:30<00:06, 3371.76it/s]\u001b[A\n",
            " 81%|████████  | 91109/112909 [00:30<00:06, 3387.30it/s]\u001b[A\n",
            " 81%|████████  | 91456/112909 [00:30<00:06, 3411.08it/s]\u001b[A\n",
            " 81%|████████▏ | 91805/112909 [00:30<00:06, 3432.68it/s]\u001b[A\n",
            " 82%|████████▏ | 92160/112909 [00:30<00:05, 3465.57it/s]\u001b[A\n",
            " 82%|████████▏ | 92507/112909 [00:30<00:05, 3461.67it/s]\u001b[A\n",
            " 82%|████████▏ | 92854/112909 [00:30<00:05, 3412.84it/s]\u001b[A\n",
            " 83%|████████▎ | 93206/112909 [00:30<00:05, 3442.16it/s]\u001b[A\n",
            " 83%|████████▎ | 93551/112909 [00:31<00:05, 3343.01it/s]\u001b[A\n",
            " 83%|████████▎ | 93887/112909 [00:31<00:05, 3291.08it/s]\u001b[A\n",
            " 83%|████████▎ | 94240/112909 [00:31<00:05, 3357.70it/s]\u001b[A\n",
            " 84%|████████▍ | 94577/112909 [00:31<00:05, 3306.16it/s]\u001b[A\n",
            " 84%|████████▍ | 94911/112909 [00:31<00:05, 3316.23it/s]\u001b[A\n",
            " 84%|████████▍ | 95264/112909 [00:31<00:05, 3375.85it/s]\u001b[A\n",
            " 85%|████████▍ | 95609/112909 [00:31<00:05, 3397.16it/s]\u001b[A\n",
            " 85%|████████▍ | 95956/112909 [00:31<00:04, 3416.60it/s]\u001b[A\n",
            " 85%|████████▌ | 96299/112909 [00:31<00:04, 3356.76it/s]\u001b[A\n",
            " 86%|████████▌ | 96643/112909 [00:31<00:04, 3379.25it/s]\u001b[A\n",
            " 86%|████████▌ | 96990/112909 [00:32<00:04, 3403.35it/s]\u001b[A\n",
            " 86%|████████▌ | 97331/112909 [00:32<00:04, 3320.33it/s]\u001b[A\n",
            " 87%|████████▋ | 97672/112909 [00:32<00:04, 3344.59it/s]\u001b[A\n",
            " 87%|████████▋ | 98013/112909 [00:32<00:04, 3363.54it/s]\u001b[A\n",
            " 87%|████████▋ | 98364/112909 [00:32<00:04, 3405.12it/s]\u001b[A\n",
            " 87%|████████▋ | 98714/112909 [00:32<00:04, 3431.97it/s]\u001b[A\n",
            " 88%|████████▊ | 99060/112909 [00:32<00:04, 3439.77it/s]\u001b[A\n",
            " 88%|████████▊ | 99405/112909 [00:32<00:03, 3436.51it/s]\u001b[A\n",
            " 88%|████████▊ | 99750/112909 [00:32<00:03, 3438.33it/s]\u001b[A\n",
            " 89%|████████▊ | 100094/112909 [00:32<00:03, 3403.55it/s]\u001b[A\n",
            " 89%|████████▉ | 100441/112909 [00:33<00:03, 3420.39it/s]\u001b[A\n",
            " 89%|████████▉ | 100784/112909 [00:33<00:03, 3365.53it/s]\u001b[A\n",
            " 90%|████████▉ | 101128/112909 [00:33<00:03, 3386.87it/s]\u001b[A\n",
            " 90%|████████▉ | 101477/112909 [00:33<00:03, 3415.98it/s]\u001b[A\n",
            " 90%|█████████ | 101819/112909 [00:33<00:03, 3408.90it/s]\u001b[A\n",
            " 90%|█████████ | 102161/112909 [00:33<00:03, 3368.63it/s]\u001b[A\n",
            " 91%|█████████ | 102509/112909 [00:33<00:03, 3399.03it/s]\u001b[A\n",
            " 91%|█████████ | 102856/112909 [00:33<00:02, 3417.36it/s]\u001b[A\n",
            " 91%|█████████▏| 103199/112909 [00:33<00:02, 3418.69it/s]\u001b[A\n",
            " 92%|█████████▏| 103541/112909 [00:33<00:02, 3334.51it/s]\u001b[A\n",
            " 92%|█████████▏| 103877/112909 [00:34<00:02, 3339.67it/s]\u001b[A\n",
            " 92%|█████████▏| 104212/112909 [00:34<00:02, 3324.85it/s]\u001b[A\n",
            " 93%|█████████▎| 104559/112909 [00:34<00:02, 3365.16it/s]\u001b[A\n",
            " 93%|█████████▎| 104911/112909 [00:34<00:02, 3409.20it/s]\u001b[A\n",
            " 93%|█████████▎| 105253/112909 [00:34<00:02, 3385.02it/s]\u001b[A\n",
            " 94%|█████████▎| 105592/112909 [00:34<00:02, 3312.00it/s]\u001b[A\n",
            " 94%|█████████▍| 105935/112909 [00:34<00:02, 3345.24it/s]\u001b[A\n",
            " 94%|█████████▍| 106283/112909 [00:34<00:01, 3382.13it/s]\u001b[A\n",
            " 94%|█████████▍| 106630/112909 [00:34<00:01, 3406.03it/s]\u001b[A\n",
            " 95%|█████████▍| 106971/112909 [00:34<00:01, 3397.08it/s]\u001b[A\n",
            " 95%|█████████▌| 107311/112909 [00:35<00:01, 3271.48it/s]\u001b[A\n",
            " 95%|█████████▌| 107640/112909 [00:35<00:01, 3217.73it/s]\u001b[A\n",
            " 96%|█████████▌| 107972/112909 [00:35<00:01, 3246.80it/s]\u001b[A\n",
            " 96%|█████████▌| 108303/112909 [00:35<00:01, 3265.31it/s]\u001b[A\n",
            " 96%|█████████▌| 108631/112909 [00:35<00:01, 3249.04it/s]\u001b[A\n",
            " 97%|█████████▋| 108963/112909 [00:35<00:01, 3269.39it/s]\u001b[A\n",
            " 97%|█████████▋| 109306/112909 [00:35<00:01, 3314.72it/s]\u001b[A\n",
            " 97%|█████████▋| 109649/112909 [00:35<00:00, 3346.46it/s]\u001b[A\n",
            " 97%|█████████▋| 109997/112909 [00:35<00:00, 3383.77it/s]\u001b[A\n",
            " 98%|█████████▊| 110342/112909 [00:35<00:00, 3403.31it/s]\u001b[A\n",
            " 98%|█████████▊| 110688/112909 [00:36<00:00, 3420.07it/s]\u001b[A\n",
            " 98%|█████████▊| 111031/112909 [00:36<00:00, 3372.88it/s]\u001b[A\n",
            " 99%|█████████▊| 111377/112909 [00:36<00:00, 3396.38it/s]\u001b[A\n",
            " 99%|█████████▉| 111722/112909 [00:36<00:00, 3410.29it/s]\u001b[A\n",
            " 99%|█████████▉| 112069/112909 [00:36<00:00, 3427.53it/s]\u001b[A\n",
            "100%|█████████▉| 112412/112909 [00:36<00:00, 3404.59it/s]\u001b[A\n",
            "100%|██████████| 112909/112909 [00:36<00:00, 3072.71it/s]\n",
            "\n",
            "  0%|          | 0/28228 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 351/28228 [00:00<00:07, 3501.75it/s]\u001b[A\n",
            "  2%|▏         | 695/28228 [00:00<00:07, 3480.54it/s]\u001b[A\n",
            "  4%|▎         | 1048/28228 [00:00<00:07, 3492.73it/s]\u001b[A\n",
            "  5%|▍         | 1396/28228 [00:00<00:07, 3488.46it/s]\u001b[A\n",
            "  6%|▌         | 1718/28228 [00:00<00:07, 3403.00it/s]\u001b[A\n",
            "  7%|▋         | 2065/28228 [00:00<00:07, 3420.68it/s]\u001b[A\n",
            "  9%|▊         | 2410/28228 [00:00<00:07, 3428.38it/s]\u001b[A\n",
            " 10%|▉         | 2758/28228 [00:00<00:07, 3441.70it/s]\u001b[A\n",
            " 11%|█         | 3100/28228 [00:00<00:07, 3432.77it/s]\u001b[A\n",
            " 12%|█▏        | 3442/28228 [00:01<00:07, 3428.84it/s]\u001b[A\n",
            " 13%|█▎        | 3790/28228 [00:01<00:07, 3443.96it/s]\u001b[A\n",
            " 15%|█▍        | 4138/28228 [00:01<00:06, 3452.85it/s]\u001b[A\n",
            " 16%|█▌        | 4484/28228 [00:01<00:06, 3453.76it/s]\u001b[A\n",
            " 17%|█▋        | 4832/28228 [00:01<00:06, 3459.62it/s]\u001b[A\n",
            " 18%|█▊        | 5176/28228 [00:01<00:07, 3192.81it/s]\u001b[A\n",
            " 20%|█▉        | 5526/28228 [00:01<00:06, 3278.08it/s]\u001b[A\n",
            " 21%|██        | 5860/28228 [00:01<00:06, 3295.22it/s]\u001b[A\n",
            " 22%|██▏       | 6216/28228 [00:01<00:06, 3368.94it/s]\u001b[A\n",
            " 23%|██▎       | 6565/28228 [00:01<00:06, 3401.83it/s]\u001b[A\n",
            " 24%|██▍       | 6909/28228 [00:02<00:06, 3411.99it/s]\u001b[A\n",
            " 26%|██▌       | 7260/28228 [00:02<00:06, 3439.26it/s]\u001b[A\n",
            " 27%|██▋       | 7611/28228 [00:02<00:05, 3458.23it/s]\u001b[A\n",
            " 28%|██▊       | 7963/28228 [00:02<00:05, 3474.04it/s]\u001b[A\n",
            " 29%|██▉       | 8311/28228 [00:02<00:05, 3411.67it/s]\u001b[A\n",
            " 31%|███       | 8653/28228 [00:02<00:05, 3357.10it/s]\u001b[A\n",
            " 32%|███▏      | 9000/28228 [00:02<00:05, 3389.13it/s]\u001b[A\n",
            " 33%|███▎      | 9350/28228 [00:02<00:05, 3420.16it/s]\u001b[A\n",
            " 34%|███▍      | 9693/28228 [00:02<00:05, 3392.95it/s]\u001b[A\n",
            " 36%|███▌      | 10036/28228 [00:02<00:05, 3401.48it/s]\u001b[A\n",
            " 37%|███▋      | 10382/28228 [00:03<00:05, 3416.32it/s]\u001b[A\n",
            " 38%|███▊      | 10725/28228 [00:03<00:05, 3419.91it/s]\u001b[A\n",
            " 39%|███▉      | 11073/28228 [00:03<00:04, 3435.81it/s]\u001b[A\n",
            " 40%|████      | 11424/28228 [00:03<00:04, 3455.23it/s]\u001b[A\n",
            " 42%|████▏     | 11770/28228 [00:03<00:04, 3393.66it/s]\u001b[A\n",
            " 43%|████▎     | 12110/28228 [00:03<00:04, 3335.13it/s]\u001b[A\n",
            " 44%|████▍     | 12451/28228 [00:03<00:04, 3355.61it/s]\u001b[A\n",
            " 45%|████▌     | 12795/28228 [00:03<00:04, 3380.07it/s]\u001b[A\n",
            " 47%|████▋     | 13141/28228 [00:03<00:04, 3401.78it/s]\u001b[A\n",
            " 48%|████▊     | 13482/28228 [00:03<00:04, 3388.74it/s]\u001b[A\n",
            " 49%|████▉     | 13822/28228 [00:04<00:04, 3382.39it/s]\u001b[A\n",
            " 50%|█████     | 14161/28228 [00:04<00:04, 3383.84it/s]\u001b[A\n",
            " 51%|█████▏    | 14500/28228 [00:04<00:04, 3298.16it/s]\u001b[A\n",
            " 53%|█████▎    | 14849/28228 [00:04<00:03, 3351.78it/s]\u001b[A\n",
            " 54%|█████▍    | 15194/28228 [00:04<00:03, 3380.39it/s]\u001b[A\n",
            " 55%|█████▌    | 15533/28228 [00:04<00:03, 3346.99it/s]\u001b[A\n",
            " 56%|█████▋    | 15880/28228 [00:04<00:03, 3380.63it/s]\u001b[A\n",
            " 58%|█████▊    | 16233/28228 [00:04<00:03, 3423.26it/s]\u001b[A\n",
            " 59%|█████▊    | 16578/28228 [00:04<00:03, 3428.82it/s]\u001b[A\n",
            " 60%|█████▉    | 16926/28228 [00:04<00:03, 3442.04it/s]\u001b[A\n",
            " 61%|██████    | 17275/28228 [00:05<00:03, 3456.20it/s]\u001b[A\n",
            " 62%|██████▏   | 17621/28228 [00:05<00:03, 3451.89it/s]\u001b[A\n",
            " 64%|██████▎   | 17976/28228 [00:05<00:02, 3477.94it/s]\u001b[A\n",
            " 65%|██████▍   | 18324/28228 [00:05<00:02, 3473.76it/s]\u001b[A\n",
            " 66%|██████▌   | 18672/28228 [00:05<00:02, 3387.25it/s]\u001b[A\n",
            " 67%|██████▋   | 19012/28228 [00:05<00:02, 3347.23it/s]\u001b[A\n",
            " 69%|██████▊   | 19352/28228 [00:05<00:02, 3361.44it/s]\u001b[A\n",
            " 70%|██████▉   | 19693/28228 [00:05<00:02, 3373.83it/s]\u001b[A\n",
            " 71%|███████   | 20036/28228 [00:05<00:02, 3388.31it/s]\u001b[A\n",
            " 72%|███████▏  | 20376/28228 [00:05<00:02, 3327.76it/s]\u001b[A\n",
            " 73%|███████▎  | 20717/28228 [00:06<00:02, 3350.15it/s]\u001b[A\n",
            " 75%|███████▍  | 21065/28228 [00:06<00:02, 3387.03it/s]\u001b[A\n",
            " 76%|███████▌  | 21415/28228 [00:06<00:01, 3417.98it/s]\u001b[A\n",
            " 77%|███████▋  | 21758/28228 [00:06<00:01, 3417.68it/s]\u001b[A\n",
            " 78%|███████▊  | 22105/28228 [00:06<00:01, 3431.80it/s]\u001b[A\n",
            " 80%|███████▉  | 22449/28228 [00:06<00:01, 3378.82it/s]\u001b[A\n",
            " 81%|████████  | 22791/28228 [00:06<00:01, 3390.19it/s]\u001b[A\n",
            " 82%|████████▏ | 23131/28228 [00:06<00:01, 3353.50it/s]\u001b[A\n",
            " 83%|████████▎ | 23471/28228 [00:06<00:01, 3365.29it/s]\u001b[A\n",
            " 84%|████████▍ | 23808/28228 [00:07<00:01, 3352.41it/s]\u001b[A\n",
            " 86%|████████▌ | 24148/28228 [00:07<00:01, 3364.86it/s]\u001b[A\n",
            " 87%|████████▋ | 24492/28228 [00:07<00:01, 3385.65it/s]\u001b[A\n",
            " 88%|████████▊ | 24849/28228 [00:07<00:00, 3438.47it/s]\u001b[A\n",
            " 89%|████████▉ | 25194/28228 [00:07<00:00, 3425.97it/s]\u001b[A\n",
            " 90%|█████████ | 25539/28228 [00:07<00:00, 3430.01it/s]\u001b[A\n",
            " 92%|█████████▏| 25883/28228 [00:07<00:00, 3363.58it/s]\u001b[A\n",
            " 93%|█████████▎| 26234/28228 [00:07<00:00, 3405.92it/s]\u001b[A\n",
            " 94%|█████████▍| 26575/28228 [00:07<00:00, 3312.44it/s]\u001b[A\n",
            " 95%|█████████▌| 26919/28228 [00:07<00:00, 3349.58it/s]\u001b[A\n",
            " 97%|█████████▋| 27269/28228 [00:08<00:00, 3391.69it/s]\u001b[A\n",
            " 98%|█████████▊| 27616/28228 [00:08<00:00, 3412.79it/s]\u001b[A\n",
            "100%|██████████| 28228/28228 [00:08<00:00, 3398.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Checkpoint2 -Normalized Vector for Sentences are created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPf82gECpA62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ab198c27-3b83-4716-99ae-b9f0f3baad3a"
      },
      "source": [
        "train_target = train\n",
        "classifier = LogisticRegression(solver='lbfgs')\n",
        "classifier.fit(xtrain_glove, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Wi4Z7_pifD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc5775d7-bd7b-482f-cd55-e62fd49b1770"
      },
      "source": [
        "print('Training LogisticRegression Classifier is complete!!')\n",
        "predictions = classifier.predict(xtest_glove)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training LogisticRegression Classifier is complete!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZOqucApn86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f32e8a83-a9b9-48f1-cb60-524a1b349614"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      4398\n",
            "           1       0.34      1.00      0.51      9688\n",
            "           2       0.00      0.00      0.00      7575\n",
            "           3       0.00      0.00      0.00      3587\n",
            "           4       0.00      0.00      0.00      2980\n",
            "\n",
            "    accuracy                           0.34     28228\n",
            "   macro avg       0.07      0.20      0.10     28228\n",
            "weighted avg       0.12      0.34      0.18     28228\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8bclrNLpq2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}