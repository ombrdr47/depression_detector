{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Input, Activation, GlobalAveragePooling1D, Flatten, Concatenate, Conv1D, MaxPooling1D,Bidirectional,TimeDistributed,Reshape,Conv2D,MaxPool2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adam\n",
    "from tensorflow.keras.preprocessing.text import one_hot, text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import fnmatch\n",
    "\n",
    "import warnings\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from ast import literal_eval\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Srijha\n",
      "[nltk_data]     Kalyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Srijha\n",
      "[nltk_data]     Kalyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Srijha\n",
      "[nltk_data]     Kalyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras import utils as np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOWS_SIZE = 10\n",
    "labels=['none','mild','moderate','moderately severe', 'severe']\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcripts_to_dataframe(directory):\n",
    "    rows_list = []\n",
    "        \n",
    "    filenames = os.listdir(directory)\n",
    "    \n",
    "    if \".DS_Store\" in filenames:\n",
    "        filenames.remove(\".DS_Store\")\n",
    "        \n",
    "    for filename in filenames:\n",
    "        transcript_path = os.path.join(directory, filename)\n",
    "        transcript = pd.read_csv(transcript_path, sep='\\t')\n",
    "        m = re.search(\"(\\d{3})_TRANSCRIPT.csv\", filename)\n",
    "        if m:\n",
    "            person_id = m.group(1)\n",
    "            p = {}\n",
    "            question = \"\"\n",
    "            answer = \"\"\n",
    "            lines = len(transcript)\n",
    "            for i in range(0, lines):\n",
    "                row = transcript.iloc[i]\n",
    "                if (row[\"speaker\"] == \"Ellie\") or (i == lines - 1):\n",
    "                    p[\"personId\"] = person_id\n",
    "                    if \"(\" in str(question):\n",
    "                        question = question[question.index(\"(\") + 1:question.index(\")\")]\n",
    "                    p[\"question\"] = question\n",
    "                    p[\"answer\"] = answer\n",
    "                    if question != \"\":\n",
    "                        rows_list.append(p)\n",
    "                    p = {}\n",
    "                    answer = \"\"\n",
    "                    question = row[\"value\"]\n",
    "                else:\n",
    "                    answer = str(answer) + \" \" + str(row[\"value\"])\n",
    "\n",
    "    all_participants = pd.DataFrame(rows_list, columns=['personId', 'question', 'answer'])\n",
    "    all_participants.to_csv(directory + 'all.csv', sep=',')\n",
    "    print(\"File was created\")\n",
    "    return all_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was created\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "data_path = \"transcripts/\"\n",
    "all_participants = transcripts_to_dataframe(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>hi i'm ellie thanks for coming in today</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>i was created to talk to people in a safe and ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>think of me as a friend i don't judge i can't ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>i'm here to learn about people and would love ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>i'll ask a few questions to get us started and...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>how are you doing today</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>that's good</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>where are you from originally</td>\n",
       "      <td>atlanta georgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>really</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>300</td>\n",
       "      <td>why'd you move to l_a</td>\n",
       "      <td>um my parents are from here um</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>300</td>\n",
       "      <td>how do you like l_a</td>\n",
       "      <td>i love it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>300</td>\n",
       "      <td>what are some things you really like about l_a</td>\n",
       "      <td>i like the weather i like the opportunities u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>300</td>\n",
       "      <td>how easy was it for you to get used to living ...</td>\n",
       "      <td>um it took a minute somewhat easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>300</td>\n",
       "      <td>what are some things you don't really like abo...</td>\n",
       "      <td>congestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>300</td>\n",
       "      <td>mhm</td>\n",
       "      <td>that's it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>300</td>\n",
       "      <td>okay</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>300</td>\n",
       "      <td>what'd you study at school</td>\n",
       "      <td>um i took up business and administration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>300</td>\n",
       "      <td>cool</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>300</td>\n",
       "      <td>are you still doing that</td>\n",
       "      <td>uh yeah i am here and there i'm on a break ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>300</td>\n",
       "      <td>what's your dream job</td>\n",
       "      <td>uh probably to open up my own business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                           question  \\\n",
       "0       300            hi i'm ellie thanks for coming in today   \n",
       "1       300  i was created to talk to people in a safe and ...   \n",
       "2       300  think of me as a friend i don't judge i can't ...   \n",
       "3       300  i'm here to learn about people and would love ...   \n",
       "4       300  i'll ask a few questions to get us started and...   \n",
       "5       300                            how are you doing today   \n",
       "6       300                                        that's good   \n",
       "7       300                      where are you from originally   \n",
       "8       300                                             really   \n",
       "9       300                              why'd you move to l_a   \n",
       "10      300                                how do you like l_a   \n",
       "11      300     what are some things you really like about l_a   \n",
       "12      300  how easy was it for you to get used to living ...   \n",
       "13      300  what are some things you don't really like abo...   \n",
       "14      300                                                mhm   \n",
       "15      300                                               okay   \n",
       "16      300                         what'd you study at school   \n",
       "17      300                                               cool   \n",
       "18      300                           are you still doing that   \n",
       "19      300                              what's your dream job   \n",
       "\n",
       "                                               answer  \n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                good  \n",
       "6                                                      \n",
       "7                                     atlanta georgia  \n",
       "8                                                      \n",
       "9                      um my parents are from here um  \n",
       "10                                          i love it  \n",
       "11   i like the weather i like the opportunities u...  \n",
       "12                  um it took a minute somewhat easy  \n",
       "13                                         congestion  \n",
       "14                                          that's it  \n",
       "15                                                     \n",
       "16           um i took up business and administration  \n",
       "17                                                     \n",
       "18   uh yeah i am here and there i'm on a break ri...  \n",
       "19             uh probably to open up my own business  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_participants.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=False):    \n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = stopwords.words(\"english\")\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text if not w in stops ]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    else:\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    \n",
    "    text = re.sub(r\"\\<\", \" \", text)\n",
    "    text = re.sub(r\"\\>\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a corpus with the words from the answers without stopwords given by the patients\n",
    "all_participants_mix = all_participants.copy() # However, if you need the original list unchanged when the new list is modified, you can use copy() method. This is called shallow copy.\n",
    "all_participants_mix[\"answer\"] = all_participants_mix.apply(lambda row: text_to_wordlist(row.answer).split(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in all_participants_mix['answer'].tolist()]\n",
    "words = set(itertools.chain(*words)) #chain('ABC', 'DEF') --> A B C D E F\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>hi i'm ellie thanks for coming in today</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>i was created to talk to people in a safe and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>think of me as a friend i don't judge i can't ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>i'm here to learn about people and would love ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>i'll ask a few questions to get us started and...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>how are you doing today</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>that's good</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>where are you from originally</td>\n",
       "      <td>[atlanta, georgia]</td>\n",
       "      <td>[1634, 1997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>really</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>300</td>\n",
       "      <td>why'd you move to l_a</td>\n",
       "      <td>[um, parent, um]</td>\n",
       "      <td>[1, 131, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>300</td>\n",
       "      <td>how do you like l_a</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[63]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>300</td>\n",
       "      <td>what are some things you really like about l_a</td>\n",
       "      <td>[like, weather, like, opportunity, um, yes]</td>\n",
       "      <td>[5, 142, 5, 334, 1, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>300</td>\n",
       "      <td>how easy was it for you to get used to living ...</td>\n",
       "      <td>[um, took, minute, somewhat, easy]</td>\n",
       "      <td>[1, 154, 527, 608, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>300</td>\n",
       "      <td>what are some things you don't really like abo...</td>\n",
       "      <td>[congestion]</td>\n",
       "      <td>[1998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>300</td>\n",
       "      <td>mhm</td>\n",
       "      <td>[that]</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personId                                           question  \\\n",
       "0       300            hi i'm ellie thanks for coming in today   \n",
       "1       300  i was created to talk to people in a safe and ...   \n",
       "2       300  think of me as a friend i don't judge i can't ...   \n",
       "3       300  i'm here to learn about people and would love ...   \n",
       "4       300  i'll ask a few questions to get us started and...   \n",
       "5       300                            how are you doing today   \n",
       "6       300                                        that's good   \n",
       "7       300                      where are you from originally   \n",
       "8       300                                             really   \n",
       "9       300                              why'd you move to l_a   \n",
       "10      300                                how do you like l_a   \n",
       "11      300     what are some things you really like about l_a   \n",
       "12      300  how easy was it for you to get used to living ...   \n",
       "13      300  what are some things you don't really like abo...   \n",
       "14      300                                                mhm   \n",
       "\n",
       "                                         answer                 t_answer  \n",
       "0                                            []                       []  \n",
       "1                                            []                       []  \n",
       "2                                            []                       []  \n",
       "3                                            []                       []  \n",
       "4                                            []                       []  \n",
       "5                                        [good]                     [16]  \n",
       "6                                            []                       []  \n",
       "7                            [atlanta, georgia]             [1634, 1997]  \n",
       "8                                            []                       []  \n",
       "9                              [um, parent, um]              [1, 131, 1]  \n",
       "10                                       [love]                     [63]  \n",
       "11  [like, weather, like, opportunity, um, yes]  [5, 142, 5, 334, 1, 39]  \n",
       "12           [um, took, minute, somewhat, easy]  [1, 154, 527, 608, 100]  \n",
       "13                                 [congestion]                   [1998]  \n",
       "14                                       [that]                     [20]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_size = WINDOWS_SIZE\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(all_participants_mix['answer']) # fit_on_texts creates the vocabulary index based on word frequency.\n",
    "#The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word \n",
    "\n",
    "tokenizer.fit_on_sequences(all_participants_mix['answer']) #texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
    "\n",
    "all_participants_mix['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix['answer'])\n",
    "all_participants_mix.head(15)\n",
    "\n",
    "#   why are the output as numbers when text_to_sequences is called?\n",
    "# the Tokenizer stores everything in the word_index during fit_on_texts. Then, when calling the texts_to_sequences method, only the top num_words are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>['good', 'atlanta', 'georgia', 'um', 'parent',...</td>\n",
       "      <td>[16, 1634, 1997, 1, 131, 1, 63, 5, 142, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>['atlanta', 'georgia', 'um', 'parent', 'um', '...</td>\n",
       "      <td>[1634, 1997, 1, 131, 1, 63, 5, 142, 5, 334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>['georgia', 'um', 'parent', 'um', 'love', 'lik...</td>\n",
       "      <td>[1997, 1, 131, 1, 63, 5, 142, 5, 334, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>['um', 'parent', 'um', 'love', 'like', 'weathe...</td>\n",
       "      <td>[1, 131, 1, 63, 5, 142, 5, 334, 1, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>['parent', 'um', 'love', 'like', 'weather', 'l...</td>\n",
       "      <td>[131, 1, 63, 5, 142, 5, 334, 1, 39, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>['um', 'love', 'like', 'weather', 'like', 'opp...</td>\n",
       "      <td>[1, 63, 5, 142, 5, 334, 1, 39, 1, 154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>['love', 'like', 'weather', 'like', 'opportuni...</td>\n",
       "      <td>[63, 5, 142, 5, 334, 1, 39, 1, 154, 527]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>300</td>\n",
       "      <td>['like', 'weather', 'like', 'opportunity', 'um...</td>\n",
       "      <td>[5, 142, 5, 334, 1, 39, 1, 154, 527, 608]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>['weather', 'like', 'opportunity', 'um', 'yes'...</td>\n",
       "      <td>[142, 5, 334, 1, 39, 1, 154, 527, 608, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>['like', 'opportunity', 'um', 'yes', 'um', 'to...</td>\n",
       "      <td>[5, 334, 1, 39, 1, 154, 527, 608, 100, 1998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>['opportunity', 'um', 'yes', 'um', 'took', 'mi...</td>\n",
       "      <td>[334, 1, 39, 1, 154, 527, 608, 100, 1998, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>300</td>\n",
       "      <td>['um', 'yes', 'um', 'took', 'minute', 'somewha...</td>\n",
       "      <td>[1, 39, 1, 154, 527, 608, 100, 1998, 20, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>300</td>\n",
       "      <td>['yes', 'um', 'took', 'minute', 'somewhat', 'e...</td>\n",
       "      <td>[39, 1, 154, 527, 608, 100, 1998, 20, 1, 154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>300</td>\n",
       "      <td>['um', 'took', 'minute', 'somewhat', 'easy', '...</td>\n",
       "      <td>[1, 154, 527, 608, 100, 1998, 20, 1, 154, 188]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>300</td>\n",
       "      <td>['took', 'minute', 'somewhat', 'easy', 'conges...</td>\n",
       "      <td>[154, 527, 608, 100, 1998, 20, 1, 154, 188, 1376]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  personId                                             answer  \\\n",
       "0            0       300  ['good', 'atlanta', 'georgia', 'um', 'parent',...   \n",
       "1            1       300  ['atlanta', 'georgia', 'um', 'parent', 'um', '...   \n",
       "2            2       300  ['georgia', 'um', 'parent', 'um', 'love', 'lik...   \n",
       "3            3       300  ['um', 'parent', 'um', 'love', 'like', 'weathe...   \n",
       "4            4       300  ['parent', 'um', 'love', 'like', 'weather', 'l...   \n",
       "5            5       300  ['um', 'love', 'like', 'weather', 'like', 'opp...   \n",
       "6            6       300  ['love', 'like', 'weather', 'like', 'opportuni...   \n",
       "7            7       300  ['like', 'weather', 'like', 'opportunity', 'um...   \n",
       "8            8       300  ['weather', 'like', 'opportunity', 'um', 'yes'...   \n",
       "9            9       300  ['like', 'opportunity', 'um', 'yes', 'um', 'to...   \n",
       "10          10       300  ['opportunity', 'um', 'yes', 'um', 'took', 'mi...   \n",
       "11          11       300  ['um', 'yes', 'um', 'took', 'minute', 'somewha...   \n",
       "12          12       300  ['yes', 'um', 'took', 'minute', 'somewhat', 'e...   \n",
       "13          13       300  ['um', 'took', 'minute', 'somewhat', 'easy', '...   \n",
       "14          14       300  ['took', 'minute', 'somewhat', 'easy', 'conges...   \n",
       "\n",
       "                                             t_answer  \n",
       "0          [16, 1634, 1997, 1, 131, 1, 63, 5, 142, 5]  \n",
       "1         [1634, 1997, 1, 131, 1, 63, 5, 142, 5, 334]  \n",
       "2            [1997, 1, 131, 1, 63, 5, 142, 5, 334, 1]  \n",
       "3              [1, 131, 1, 63, 5, 142, 5, 334, 1, 39]  \n",
       "4              [131, 1, 63, 5, 142, 5, 334, 1, 39, 1]  \n",
       "5              [1, 63, 5, 142, 5, 334, 1, 39, 1, 154]  \n",
       "6            [63, 5, 142, 5, 334, 1, 39, 1, 154, 527]  \n",
       "7           [5, 142, 5, 334, 1, 39, 1, 154, 527, 608]  \n",
       "8         [142, 5, 334, 1, 39, 1, 154, 527, 608, 100]  \n",
       "9        [5, 334, 1, 39, 1, 154, 527, 608, 100, 1998]  \n",
       "10      [334, 1, 39, 1, 154, 527, 608, 100, 1998, 20]  \n",
       "11        [1, 39, 1, 154, 527, 608, 100, 1998, 20, 1]  \n",
       "12      [39, 1, 154, 527, 608, 100, 1998, 20, 1, 154]  \n",
       "13     [1, 154, 527, 608, 100, 1998, 20, 1, 154, 188]  \n",
       "14  [154, 527, 608, 100, 1998, 20, 1, 154, 188, 1376]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_lp = pd.read_csv('phrases_lp.csv', sep='\\t', converters={\"t_answer\": literal_eval}) \n",
    "phrases_lp.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "def load_avec_dataset_file(path,score_column):\n",
    "    ds = pd.read_csv(path, sep=',')\n",
    "    ds['level'] = pd.cut(ds[score_column], bins=[-1,0,5,10,15,25], labels=[0,1,2,3,4])  #cut function used to segregate array into bins 5 levels - 'none','mild','moderate','moderately severe', 'severe'\n",
    "    ds['PHQ8_Score'] = ds[score_column]\n",
    "    ds['cat_level'] = to_categorical(ds['level'], num_classes).tolist() #categorical levels \n",
    "    ds = ds[['Participant_ID', 'level', 'cat_level', 'PHQ8_Score','Gender']] \n",
    "    ds = ds.astype({\"Participant_ID\": float, \"level\": int, 'PHQ8_Score': int})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: train= 107, dev= 35, test=47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>level</th>\n",
       "      <th>cat_level</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant_ID  level                  cat_level  PHQ8_Score  Gender\n",
       "0           303.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       0\n",
       "1           304.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           6       0\n",
       "2           305.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           7       1\n",
       "3           310.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           4       1\n",
       "4           312.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           2       1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = load_avec_dataset_file('train_split_Depression_AVEC2017 (1).csv','PHQ8_Score')\n",
    "dev = load_avec_dataset_file('dev_split_Depression_AVEC2017.csv','PHQ8_Score')\n",
    "test = load_avec_dataset_file('full_test_split.csv','PHQ8_Score')\n",
    "print(\"Size: train= {}, dev= {}, test={}\".format(len(train), len(dev), len(test)))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size = 189\n"
     ]
    }
   ],
   "source": [
    "ds_total = pd.concat([dev,train,test])\n",
    "total_phq8 = len(ds_total)\n",
    "print(\"Total size = {}\".format(total_phq8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was created\n"
     ]
    }
   ],
   "source": [
    "ds_total.to_csv('ds_total.csv', sep='\\t')\n",
    "print(\"File was created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_phq_level(ds):\n",
    "    none_ds = ds[ds['level']==0]\n",
    "    mild_ds = ds[ds['level']==1]\n",
    "    moderate_ds = ds[ds['level']==2]\n",
    "    moderate_severe_ds = ds[ds['level']==3]\n",
    "    severe_ds = ds[ds['level']==4]\n",
    "    return (none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity per none_ds: 26, mild_ds: 70, moderate_ds 47, moderate_severe_ds: 24, severe_ds 22\n"
     ]
    }
   ],
   "source": [
    "none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_total)\n",
    "print(\"Quantity per none_ds: {}, mild_ds: {}, moderate_ds {}, moderate_severe_ds: {}, severe_ds {}\".format(len(none_ds), len(mild_ds), len(moderate_ds), len(moderate_severe_ds), len(severe_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_none_ds = ds_total[ds_total['level']==0]\n",
    "b_mild_ds = ds_total[ds_total['level']==1].sample(26)\n",
    "b_moderate_ds = ds_total[ds_total['level']==2].sample(26)\n",
    "b_moderate_severe_ds = ds_total[ds_total['level']==3]\n",
    "b_severe_ds = ds_total[ds_total['level']==4]\n",
    "\n",
    "ds_total_b = pd.concat([b_none_ds, b_mild_ds, b_moderate_ds, b_moderate_severe_ds, b_severe_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_= ds_total_b.to_csv('ds_total_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lp = pd.merge(ds_total, phrases_lp,left_on='Participant_ID', right_on='personId')\n",
    "ds_lp.drop(ds_lp[ds_lp[\"t_answer\"].map(len) < 10].index, inplace = True)\n",
    "ds_lp_b = pd.merge(ds_total_b, phrases_lp,left_on='Participant_ID', right_on='personId')\n",
    "ds_lp_b.drop(ds_lp_b[ds_lp_b[\"t_answer\"].map(len) < 10].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_instances(ds, split_in = [70,14,16]):\n",
    "    ds_shuffled = ds.sample(frac=1)\n",
    "    none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_shuffled)\n",
    "    eq_ds = dict()\n",
    "    prev_none = prev_mild = prev_moderate = prev_moderate_severe = prev_severe = 0\n",
    "    split = split_in\n",
    "    for p in split:\n",
    "        last_none = min(len(none_ds), prev_none + round(len(none_ds) * p/100))\n",
    "        last_mild = min(len(mild_ds), prev_mild + round(len(mild_ds) * p/100))\n",
    "        last_moderate = min(len(moderate_ds), prev_moderate + round(len(moderate_ds) * p/100))\n",
    "        last_moderate_severe = min(len(moderate_severe_ds), prev_moderate_severe + round(len(moderate_severe_ds) * p/100))\n",
    "        last_severe = min(len(severe_ds), prev_severe + round(len(severe_ds) * p/100))  \n",
    "        eq_ds['d'+str(p)] = pd.concat([none_ds[prev_none: last_none], mild_ds[prev_mild: last_mild], moderate_ds[prev_moderate: last_moderate], moderate_severe_ds[prev_moderate_severe: last_moderate_severe], severe_ds[prev_severe: last_severe]])\n",
    "        prev_none = last_none\n",
    "        prev_mild = last_mild\n",
    "        prev_moderate = last_moderate\n",
    "        prev_moderate_severe = last_moderate_severe\n",
    "        prev_severe = last_severe  \n",
    "    return (eq_ds['d70'], eq_ds['d14'], eq_ds['d16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lp, dev_lp, test_lp = distribute_instances(ds_lp)\n",
    "train_lp_b, dev_lp_b, test_lp_b = distribute_instances(ds_lp_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_embedding_matrix(tokenizer):\n",
    "    vocab_size = len(tokenizer.word_index) # tokenizer.word_index is the list that consist of all the unique words\n",
    "    embedding_matrix = np.zeros((vocab_size+1, 100)) # creating an embedding matrix\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:        \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7374, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_lp = fill_embedding_matrix(tokenizer)\n",
    "embedding_matrix_lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>level</th>\n",
       "      <th>cat_level</th>\n",
       "      <th>PHQ8_Score</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>personId</th>\n",
       "      <th>answer</th>\n",
       "      <th>t_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95039</th>\n",
       "      <td>463.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>121679</td>\n",
       "      <td>463</td>\n",
       "      <td>['saleing', 'family', 'pretty', 'fun', 'going'...</td>\n",
       "      <td>[6855, 61, 23, 127, 40, 38, 1441, 1238, 616, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126186</th>\n",
       "      <td>411.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82912</td>\n",
       "      <td>411</td>\n",
       "      <td>['incredible', 'healing', 'thing', 'believe', ...</td>\n",
       "      <td>[1129, 3866, 9, 343, 2, 1, 2, 12, 553, 944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58187</th>\n",
       "      <td>370.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51323</td>\n",
       "      <td>370</td>\n",
       "      <td>['even', 'though', 'could', 'get', 'full', 'ei...</td>\n",
       "      <td>[78, 217, 37, 18, 705, 363, 213, 2257, 77, 50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124592</th>\n",
       "      <td>408.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80014</td>\n",
       "      <td>408</td>\n",
       "      <td>['cannot', 'think', 'sorry', 'laughter', 'guil...</td>\n",
       "      <td>[68, 12, 374, 8, 305, 12, 70, 28, 305, 1177]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115800</th>\n",
       "      <td>361.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40165</td>\n",
       "      <td>361</td>\n",
       "      <td>['like', 'kid', 'candy', 'store', 'get', 'play...</td>\n",
       "      <td>[5, 116, 2941, 857, 18, 288, 38, 4279, 1, 7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Participant_ID  level                  cat_level  PHQ8_Score  Gender  \\\n",
       "95039            463.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       1   \n",
       "126186           411.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       0   \n",
       "58187            370.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       1   \n",
       "124592           408.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       0   \n",
       "115800           361.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       1   \n",
       "\n",
       "        Unnamed: 0  personId  \\\n",
       "95039       121679       463   \n",
       "126186       82912       411   \n",
       "58187        51323       370   \n",
       "124592       80014       408   \n",
       "115800       40165       361   \n",
       "\n",
       "                                                   answer  \\\n",
       "95039   ['saleing', 'family', 'pretty', 'fun', 'going'...   \n",
       "126186  ['incredible', 'healing', 'thing', 'believe', ...   \n",
       "58187   ['even', 'though', 'could', 'get', 'full', 'ei...   \n",
       "124592  ['cannot', 'think', 'sorry', 'laughter', 'guil...   \n",
       "115800  ['like', 'kid', 'candy', 'store', 'get', 'play...   \n",
       "\n",
       "                                               t_answer  \n",
       "95039   [6855, 61, 23, 127, 40, 38, 1441, 1238, 616, 2]  \n",
       "126186      [1129, 3866, 9, 343, 2, 1, 2, 12, 553, 944]  \n",
       "58187    [78, 217, 37, 18, 705, 363, 213, 2257, 77, 50]  \n",
       "124592     [68, 12, 374, 8, 305, 12, 70, 28, 305, 1177]  \n",
       "115800     [5, 116, 2941, 857, 18, 288, 38, 4279, 1, 7]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lp['t_answer']\n",
    "train_lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a =np.stack(train_lp['t_answer'])\n",
    "dev_a = np.stack(dev_lp['t_answer'])\n",
    "train_y = np.stack(train_lp['cat_level'], axis=0)\n",
    "dev_y = np.stack(dev_lp['cat_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_b = np.stack(train_lp_b['t_answer'], axis=0)\n",
    "dev_a_b = np.stack(dev_lp_b['t_answer'], axis=0)\n",
    "train_y_b = np.stack(train_lp_b['cat_level'], axis=0)\n",
    "dev_y_b = np.stack(dev_lp_b['cat_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(history, title=\"Model Accuracy\"):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(history, title=\"Model Loss\"):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_losses(history1, history2, name1=\"Red 1\", name2=\"Red 2\", title=\"Graph title\"):\n",
    "    plt.plot(history1.history['loss'], color=\"green\")\n",
    "    plt.plot(history1.history['val_loss'], 'r--', color=\"green\")\n",
    "    plt.plot(history2.history['loss'], color=\"blue\")\n",
    "    plt.plot(history2.history['val_loss'], 'r--', color=\"blue\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train ' + name1, 'Val ' + name1, \n",
    "                'Train ' + name2, 'Val ' + name2],\n",
    "               loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_accs(history1, history2, name1=\"Red 1\",\n",
    "                      name2=\"Red 2\", title=\"Graph title\"):\n",
    "    \"\"\"Compara accuracies de dos entrenamientos con nombres name1 y name2\"\"\"\n",
    "    plt.plot(history1.history['acc'], color=\"green\")\n",
    "    plt.plot(history1.history['val_acc'], 'r--', color=\"green\")\n",
    "    plt.plot(history2.history['acc'], color=\"blue\")\n",
    "    plt.plot(history2.history['val_acc'], 'r--', color=\"blue\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train ' + name1, 'Val ' + name1, \n",
    "                'Train ' + name2, 'Val ' + name2], \n",
    "               loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_compare_multiple_metrics(history_array, names, colors, title=\"Graph title\", metric='acc'):  \n",
    "    legend = []\n",
    "    for i in range(0, len(history_array)):\n",
    "        plt.plot(history_array[i].history[metric], color=colors[i])\n",
    "        plt.plot(history_array[i].history['val_' + metric], 'r--', color=colors[i])\n",
    "        legend.append('Train ' + names[i])\n",
    "        legend.append('Val ' + names[i])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')   \n",
    "    plt.axis\n",
    "    plt.legend(legend, \n",
    "               loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 10, 100)           737400    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 10, 100)           400       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10, 256)           25856     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10, 256)           65792     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 12805     \n",
      "=================================================================\n",
      "Total params: 922,653\n",
      "Trainable params: 185,053\n",
      "Non-trainable params: 737,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bt = BatchNormalization()(answer_emb1)\n",
    "lstm = LSTM(embedding_size_glove, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(bt)\n",
    "\n",
    "dense1 = Dense(units=256, activation=\"relu\")(lstm)\n",
    "dense2 = Dense(units=256, activation=\"relu\")(dense1)\n",
    "\n",
    "flatten = Flatten()(dense2)\n",
    "\n",
    "out = Dense(5,  activation='softmax')(flatten)\n",
    "\n",
    "model = Model(inputs=[answer_inp], outputs=[out])\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98795 samples, validate on 19760 samples\n",
      "Epoch 1/30\n",
      "98795/98795 [==============================] - 42s 430us/sample - loss: 1.4518 - accuracy: 0.3713 - val_loss: 1.3246 - val_accuracy: 0.4396\n",
      "Epoch 2/30\n",
      "98795/98795 [==============================] - 37s 378us/sample - loss: 1.2637 - accuracy: 0.4742 - val_loss: 1.0869 - val_accuracy: 0.5647\n",
      "Epoch 3/30\n",
      "98795/98795 [==============================] - 38s 388us/sample - loss: 1.0989 - accuracy: 0.5550 - val_loss: 0.9280 - val_accuracy: 0.6346\n",
      "Epoch 4/30\n",
      "98795/98795 [==============================] - 37s 377us/sample - loss: 0.9874 - accuracy: 0.6067 - val_loss: 0.8028 - val_accuracy: 0.6962\n",
      "Epoch 5/30\n",
      "98795/98795 [==============================] - 39s 397us/sample - loss: 0.9080 - accuracy: 0.6431 - val_loss: 0.7263 - val_accuracy: 0.7285\n",
      "Epoch 6/30\n",
      "98795/98795 [==============================] - 40s 406us/sample - loss: 0.8435 - accuracy: 0.6696 - val_loss: 0.6364 - val_accuracy: 0.7648\n",
      "Epoch 7/30\n",
      "98795/98795 [==============================] - 43s 440us/sample - loss: 0.7945 - accuracy: 0.6942 - val_loss: 0.5865 - val_accuracy: 0.7818\n",
      "Epoch 8/30\n",
      "98795/98795 [==============================] - 44s 445us/sample - loss: 0.7486 - accuracy: 0.7095 - val_loss: 0.5323 - val_accuracy: 0.8110\n",
      "Epoch 9/30\n",
      "98795/98795 [==============================] - 43s 436us/sample - loss: 0.7112 - accuracy: 0.7257 - val_loss: 0.4996 - val_accuracy: 0.8193\n",
      "Epoch 10/30\n",
      "98795/98795 [==============================] - 42s 430us/sample - loss: 0.6806 - accuracy: 0.7384 - val_loss: 0.4711 - val_accuracy: 0.8301\n",
      "Epoch 11/30\n",
      "98795/98795 [==============================] - 43s 439us/sample - loss: 0.6517 - accuracy: 0.7516 - val_loss: 0.4337 - val_accuracy: 0.8457\n",
      "Epoch 12/30\n",
      "98795/98795 [==============================] - 44s 442us/sample - loss: 0.6249 - accuracy: 0.7629 - val_loss: 0.3946 - val_accuracy: 0.8624\n",
      "Epoch 13/30\n",
      "98795/98795 [==============================] - 44s 447us/sample - loss: 0.5988 - accuracy: 0.7722 - val_loss: 0.3768 - val_accuracy: 0.8705\n",
      "Epoch 14/30\n",
      "98795/98795 [==============================] - 45s 454us/sample - loss: 0.5776 - accuracy: 0.7821 - val_loss: 0.3450 - val_accuracy: 0.8832\n",
      "Epoch 15/30\n",
      "98795/98795 [==============================] - 45s 453us/sample - loss: 0.5551 - accuracy: 0.7907 - val_loss: 0.3241 - val_accuracy: 0.8897\n",
      "Epoch 16/30\n",
      "98795/98795 [==============================] - 44s 450us/sample - loss: 0.5343 - accuracy: 0.7987 - val_loss: 0.3069 - val_accuracy: 0.8968\n",
      "Epoch 17/30\n",
      "98795/98795 [==============================] - 45s 452us/sample - loss: 0.5185 - accuracy: 0.8060 - val_loss: 0.2870 - val_accuracy: 0.9042\n",
      "Epoch 18/30\n",
      "98795/98795 [==============================] - 41s 420us/sample - loss: 0.5001 - accuracy: 0.8138 - val_loss: 0.2798 - val_accuracy: 0.9044\n",
      "Epoch 19/30\n",
      "98795/98795 [==============================] - 40s 409us/sample - loss: 0.4854 - accuracy: 0.8192 - val_loss: 0.2639 - val_accuracy: 0.9120\n",
      "Epoch 20/30\n",
      "98795/98795 [==============================] - 40s 403us/sample - loss: 0.4750 - accuracy: 0.8242 - val_loss: 0.2478 - val_accuracy: 0.9177\n",
      "Epoch 21/30\n",
      "98795/98795 [==============================] - 40s 403us/sample - loss: 0.4607 - accuracy: 0.8272 - val_loss: 0.2323 - val_accuracy: 0.9222\n",
      "Epoch 22/30\n",
      "98795/98795 [==============================] - 41s 412us/sample - loss: 0.4473 - accuracy: 0.8346 - val_loss: 0.2227 - val_accuracy: 0.9259\n",
      "Epoch 23/30\n",
      "98795/98795 [==============================] - 43s 434us/sample - loss: 0.4332 - accuracy: 0.8397 - val_loss: 0.2087 - val_accuracy: 0.9305\n",
      "Epoch 24/30\n",
      "98795/98795 [==============================] - 41s 415us/sample - loss: 0.4213 - accuracy: 0.8450 - val_loss: 0.2014 - val_accuracy: 0.9324\n",
      "Epoch 25/30\n",
      "98795/98795 [==============================] - 40s 405us/sample - loss: 0.4136 - accuracy: 0.8474 - val_loss: 0.1977 - val_accuracy: 0.9317\n",
      "Epoch 26/30\n",
      "98795/98795 [==============================] - 40s 409us/sample - loss: 0.4067 - accuracy: 0.8498 - val_loss: 0.1799 - val_accuracy: 0.9426\n",
      "Epoch 27/30\n",
      "98795/98795 [==============================] - 40s 408us/sample - loss: 0.3928 - accuracy: 0.8548 - val_loss: 0.1788 - val_accuracy: 0.9435\n",
      "Epoch 28/30\n",
      "98795/98795 [==============================] - 41s 417us/sample - loss: 0.3861 - accuracy: 0.8586 - val_loss: 0.1671 - val_accuracy: 0.9450\n",
      "Epoch 29/30\n",
      "98795/98795 [==============================] - 41s 413us/sample - loss: 0.3781 - accuracy: 0.8603 - val_loss: 0.1663 - val_accuracy: 0.9452\n",
      "Epoch 30/30\n",
      "98795/98795 [==============================] - 42s 429us/sample - loss: 0.3710 - accuracy: 0.8634 - val_loss: 0.1663 - val_accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "model_glove_lstm_hist = model.fit(train_a, train_y, validation_data=(dev_a, dev_y), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_glove_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64316 samples, validate on 12864 samples\n",
      "Epoch 1/30\n",
      "64316/64316 [==============================] - 25s 384us/sample - loss: 0.3857 - accuracy: 0.8607 - val_loss: 0.0740 - val_accuracy: 0.9815\n",
      "Epoch 2/30\n",
      "64316/64316 [==============================] - 25s 382us/sample - loss: 0.3376 - accuracy: 0.8788 - val_loss: 0.0669 - val_accuracy: 0.9842\n",
      "Epoch 3/30\n",
      "64316/64316 [==============================] - 24s 375us/sample - loss: 0.3120 - accuracy: 0.8879 - val_loss: 0.0599 - val_accuracy: 0.9845\n",
      "Epoch 4/30\n",
      "64316/64316 [==============================] - 24s 373us/sample - loss: 0.2871 - accuracy: 0.8972 - val_loss: 0.0559 - val_accuracy: 0.9854\n",
      "Epoch 5/30\n",
      "64316/64316 [==============================] - 24s 378us/sample - loss: 0.2786 - accuracy: 0.9013 - val_loss: 0.0551 - val_accuracy: 0.9854\n",
      "Epoch 6/30\n",
      "64316/64316 [==============================] - 25s 382us/sample - loss: 0.2646 - accuracy: 0.9046 - val_loss: 0.0542 - val_accuracy: 0.9852\n",
      "Epoch 7/30\n",
      "64316/64316 [==============================] - 25s 393us/sample - loss: 0.2565 - accuracy: 0.9080 - val_loss: 0.0499 - val_accuracy: 0.9864\n",
      "Epoch 8/30\n",
      "64316/64316 [==============================] - 26s 410us/sample - loss: 0.2457 - accuracy: 0.9132 - val_loss: 0.0482 - val_accuracy: 0.9855\n",
      "Epoch 9/30\n",
      "64316/64316 [==============================] - 27s 426us/sample - loss: 0.2385 - accuracy: 0.9138 - val_loss: 0.0496 - val_accuracy: 0.9852\n",
      "Epoch 10/30\n",
      "64316/64316 [==============================] - 30s 461us/sample - loss: 0.2344 - accuracy: 0.9162 - val_loss: 0.0469 - val_accuracy: 0.9873\n",
      "Epoch 11/30\n",
      "64316/64316 [==============================] - 33s 513us/sample - loss: 0.2197 - accuracy: 0.9232 - val_loss: 0.0473 - val_accuracy: 0.9861\n",
      "Epoch 12/30\n",
      "64316/64316 [==============================] - 33s 512us/sample - loss: 0.2254 - accuracy: 0.9194 - val_loss: 0.0451 - val_accuracy: 0.9864\n",
      "Epoch 13/30\n",
      "64316/64316 [==============================] - 31s 487us/sample - loss: 0.2134 - accuracy: 0.9245 - val_loss: 0.0421 - val_accuracy: 0.9869\n",
      "Epoch 14/30\n",
      "64316/64316 [==============================] - 33s 508us/sample - loss: 0.2127 - accuracy: 0.9247 - val_loss: 0.0427 - val_accuracy: 0.9862\n",
      "Epoch 15/30\n",
      "64316/64316 [==============================] - 33s 508us/sample - loss: 0.2052 - accuracy: 0.9274 - val_loss: 0.0414 - val_accuracy: 0.9867\n",
      "Epoch 16/30\n",
      "64316/64316 [==============================] - 33s 514us/sample - loss: 0.1968 - accuracy: 0.9301 - val_loss: 0.0438 - val_accuracy: 0.9877\n",
      "Epoch 17/30\n",
      "64316/64316 [==============================] - 29s 444us/sample - loss: 0.1973 - accuracy: 0.9300 - val_loss: 0.0406 - val_accuracy: 0.9875\n",
      "Epoch 18/30\n",
      "64316/64316 [==============================] - 25s 388us/sample - loss: 0.1934 - accuracy: 0.9316 - val_loss: 0.0415 - val_accuracy: 0.9876\n",
      "Epoch 19/30\n",
      "64316/64316 [==============================] - 26s 411us/sample - loss: 0.1913 - accuracy: 0.9319 - val_loss: 0.0377 - val_accuracy: 0.9877\n",
      "Epoch 20/30\n",
      "64316/64316 [==============================] - 26s 411us/sample - loss: 0.1863 - accuracy: 0.9339 - val_loss: 0.0357 - val_accuracy: 0.9900\n",
      "Epoch 21/30\n",
      "64316/64316 [==============================] - 27s 417us/sample - loss: 0.1831 - accuracy: 0.9351 - val_loss: 0.0367 - val_accuracy: 0.9900\n",
      "Epoch 22/30\n",
      "64316/64316 [==============================] - 27s 417us/sample - loss: 0.1844 - accuracy: 0.9349 - val_loss: 0.0389 - val_accuracy: 0.9885\n",
      "Epoch 23/30\n",
      "64316/64316 [==============================] - 26s 409us/sample - loss: 0.1832 - accuracy: 0.9356 - val_loss: 0.0361 - val_accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "model_glove_lstm_hist_b = model.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_glove_lstm_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 100)           737400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 925,741\n",
      "Trainable params: 188,141\n",
      "Non-trainable params: 737,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n",
    "\n",
    "lstm1 = LSTM(embedding_size_glove, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(answer_emb1)\n",
    "lstm2 = LSTM(embedding_size_glove, dropout=0.2, recurrent_dropout=0.2)(lstm1)\n",
    "\n",
    "X = Dropout(0.2)(lstm2)\n",
    "bt = BatchNormalization()(X)\n",
    "dense1 = Dense(units=256, activation=\"relu\")(bt)\n",
    "\n",
    "out = Dense(5,  activation='softmax')(dense1)\n",
    "\n",
    "model_2lstm = Model(inputs=[answer_inp], outputs=[out])\n",
    "model_2lstm.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64316 samples, validate on 12864 samples\n",
      "Epoch 1/30\n",
      "64316/64316 [==============================] - 46s 720us/sample - loss: 1.6160 - accuracy: 0.2439 - val_loss: 1.5850 - val_accuracy: 0.2614\n",
      "Epoch 2/30\n",
      "64316/64316 [==============================] - 37s 569us/sample - loss: 1.5728 - accuracy: 0.2797 - val_loss: 1.5526 - val_accuracy: 0.2996\n",
      "Epoch 3/30\n",
      "64316/64316 [==============================] - 36s 566us/sample - loss: 1.5345 - accuracy: 0.3125 - val_loss: 1.4871 - val_accuracy: 0.3500\n",
      "Epoch 4/30\n",
      "64316/64316 [==============================] - 39s 601us/sample - loss: 1.4722 - accuracy: 0.3624 - val_loss: 1.3962 - val_accuracy: 0.4082\n",
      "Epoch 5/30\n",
      "64316/64316 [==============================] - 42s 651us/sample - loss: 1.3924 - accuracy: 0.4111 - val_loss: 1.2893 - val_accuracy: 0.4737\n",
      "Epoch 6/30\n",
      "64316/64316 [==============================] - 51s 799us/sample - loss: 1.3083 - accuracy: 0.4600 - val_loss: 1.1948 - val_accuracy: 0.5283\n",
      "Epoch 7/30\n",
      "64316/64316 [==============================] - 43s 671us/sample - loss: 1.2356 - accuracy: 0.4995 - val_loss: 1.1230 - val_accuracy: 0.5570\n",
      "Epoch 8/30\n",
      "64316/64316 [==============================] - 39s 602us/sample - loss: 1.1649 - accuracy: 0.5338 - val_loss: 1.0373 - val_accuracy: 0.5962\n",
      "Epoch 9/30\n",
      "64316/64316 [==============================] - 40s 618us/sample - loss: 1.1095 - accuracy: 0.5594 - val_loss: 0.9695 - val_accuracy: 0.6342\n",
      "Epoch 10/30\n",
      "64316/64316 [==============================] - 39s 604us/sample - loss: 1.0589 - accuracy: 0.5819 - val_loss: 0.9041 - val_accuracy: 0.6623\n",
      "Epoch 11/30\n",
      "64316/64316 [==============================] - 39s 607us/sample - loss: 1.0135 - accuracy: 0.6025 - val_loss: 0.8629 - val_accuracy: 0.6779\n",
      "Epoch 12/30\n",
      "64316/64316 [==============================] - 39s 607us/sample - loss: 0.9810 - accuracy: 0.6180 - val_loss: 0.8039 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "64316/64316 [==============================] - 39s 603us/sample - loss: 0.9440 - accuracy: 0.6347 - val_loss: 0.7571 - val_accuracy: 0.7258\n",
      "Epoch 14/30\n",
      "64316/64316 [==============================] - 38s 590us/sample - loss: 0.9136 - accuracy: 0.6479 - val_loss: 0.7251 - val_accuracy: 0.7381\n",
      "Epoch 15/30\n",
      "64316/64316 [==============================] - 38s 588us/sample - loss: 0.8952 - accuracy: 0.6531 - val_loss: 0.7220 - val_accuracy: 0.7394\n",
      "Epoch 16/30\n",
      "64316/64316 [==============================] - 39s 600us/sample - loss: 0.8677 - accuracy: 0.6684 - val_loss: 0.6731 - val_accuracy: 0.7622\n",
      "Epoch 17/30\n",
      "64316/64316 [==============================] - 38s 597us/sample - loss: 0.8499 - accuracy: 0.6745 - val_loss: 0.6489 - val_accuracy: 0.7662\n",
      "Epoch 18/30\n",
      "64316/64316 [==============================] - 38s 593us/sample - loss: 0.8316 - accuracy: 0.6834 - val_loss: 0.6216 - val_accuracy: 0.7819\n",
      "Epoch 19/30\n",
      "64316/64316 [==============================] - 38s 591us/sample - loss: 0.8187 - accuracy: 0.6876 - val_loss: 0.5995 - val_accuracy: 0.7883\n",
      "Epoch 20/30\n",
      "64316/64316 [==============================] - 39s 601us/sample - loss: 0.7972 - accuracy: 0.6981 - val_loss: 0.5764 - val_accuracy: 0.7988\n",
      "Epoch 21/30\n",
      "64316/64316 [==============================] - 38s 592us/sample - loss: 0.7890 - accuracy: 0.7004 - val_loss: 0.5561 - val_accuracy: 0.8085\n",
      "Epoch 22/30\n",
      "64316/64316 [==============================] - 38s 593us/sample - loss: 0.7725 - accuracy: 0.7078 - val_loss: 0.5399 - val_accuracy: 0.8124\n",
      "Epoch 23/30\n",
      "64316/64316 [==============================] - 38s 596us/sample - loss: 0.7544 - accuracy: 0.7158 - val_loss: 0.5256 - val_accuracy: 0.8195\n",
      "Epoch 24/30\n",
      "64316/64316 [==============================] - 38s 590us/sample - loss: 0.7506 - accuracy: 0.7164 - val_loss: 0.5178 - val_accuracy: 0.8260\n",
      "Epoch 25/30\n",
      "64316/64316 [==============================] - 48s 746us/sample - loss: 0.7395 - accuracy: 0.7215 - val_loss: 0.5024 - val_accuracy: 0.8343\n",
      "Epoch 26/30\n",
      "64316/64316 [==============================] - 48s 752us/sample - loss: 0.7238 - accuracy: 0.7275 - val_loss: 0.4816 - val_accuracy: 0.8318\n",
      "Epoch 27/30\n",
      "64316/64316 [==============================] - 47s 729us/sample - loss: 0.7165 - accuracy: 0.7280 - val_loss: 0.4697 - val_accuracy: 0.8416\n",
      "Epoch 28/30\n",
      "64316/64316 [==============================] - 36s 565us/sample - loss: 0.7115 - accuracy: 0.7332 - val_loss: 0.4631 - val_accuracy: 0.8427\n",
      "Epoch 29/30\n",
      "64316/64316 [==============================] - 38s 590us/sample - loss: 0.6968 - accuracy: 0.7378 - val_loss: 0.4420 - val_accuracy: 0.8547\n",
      "Epoch 30/30\n",
      "64316/64316 [==============================] - 38s 587us/sample - loss: 0.6938 - accuracy: 0.7409 - val_loss: 0.4276 - val_accuracy: 0.8568\n"
     ]
    }
   ],
   "source": [
    "model_glove_2lstm_b_hist = model_2lstm.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2lstm.save('model_2lstm_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 100)           737400    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 100)           20100     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               256256    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,175,841\n",
      "Trainable params: 438,441\n",
      "Non-trainable params: 737,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# main model\n",
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n",
    "\n",
    "bi_lstm =  Bidirectional (LSTM (embedding_size_glove,return_sequences=True,dropout=0.50),merge_mode='concat')(answer_emb1)\n",
    "model_bi1 = TimeDistributed(Dense(embedding_size_glove,activation='relu'))(bi_lstm) #TimeDistributed method is used to apply a Dense layer to each of the time-steps independently. We used Dropout and l2_reg regularizers to reduce overfitting.\n",
    "model_bi2 = Flatten()(model_bi1)\n",
    "model_bi3 = Dense(256,activation='relu')(model_bi2)\n",
    "output = Dense(5,activation='softmax')(model_bi3)\n",
    "model_bi = Model(answer_inp,output)\n",
    "model_bi.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model_bi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64316 samples, validate on 12864 samples\n",
      "Epoch 1/30\n",
      "64316/64316 [==============================] - 33s 508us/sample - loss: 1.5807 - accuracy: 0.2648 - val_loss: 1.5552 - val_accuracy: 0.2844\n",
      "Epoch 2/30\n",
      "64316/64316 [==============================] - 26s 397us/sample - loss: 1.5456 - accuracy: 0.2996 - val_loss: 1.5061 - val_accuracy: 0.3270\n",
      "Epoch 3/30\n",
      "64316/64316 [==============================] - 26s 399us/sample - loss: 1.4966 - accuracy: 0.3356 - val_loss: 1.4572 - val_accuracy: 0.3714\n",
      "Epoch 4/30\n",
      "64316/64316 [==============================] - 26s 402us/sample - loss: 1.4332 - accuracy: 0.3801 - val_loss: 1.3715 - val_accuracy: 0.4317\n",
      "Epoch 5/30\n",
      "64316/64316 [==============================] - 26s 401us/sample - loss: 1.3689 - accuracy: 0.4199 - val_loss: 1.2754 - val_accuracy: 0.4777\n",
      "Epoch 6/30\n",
      "64316/64316 [==============================] - 26s 401us/sample - loss: 1.3079 - accuracy: 0.4546 - val_loss: 1.1951 - val_accuracy: 0.5257\n",
      "Epoch 7/30\n",
      "64316/64316 [==============================] - 26s 397us/sample - loss: 1.2478 - accuracy: 0.4859 - val_loss: 1.1385 - val_accuracy: 0.5548\n",
      "Epoch 8/30\n",
      "64316/64316 [==============================] - 26s 408us/sample - loss: 1.1968 - accuracy: 0.5119 - val_loss: 1.0572 - val_accuracy: 0.5925\n",
      "Epoch 9/30\n",
      "64316/64316 [==============================] - 26s 404us/sample - loss: 1.1567 - accuracy: 0.5306 - val_loss: 1.0014 - val_accuracy: 0.6179\n",
      "Epoch 10/30\n",
      "64316/64316 [==============================] - 27s 421us/sample - loss: 1.1184 - accuracy: 0.5480 - val_loss: 0.9522 - val_accuracy: 0.6392\n",
      "Epoch 11/30\n",
      "64316/64316 [==============================] - 27s 418us/sample - loss: 1.0781 - accuracy: 0.5677 - val_loss: 0.9167 - val_accuracy: 0.6549\n",
      "Epoch 12/30\n",
      "64316/64316 [==============================] - 27s 413us/sample - loss: 1.0429 - accuracy: 0.5822 - val_loss: 0.8785 - val_accuracy: 0.6716\n",
      "Epoch 13/30\n",
      "64316/64316 [==============================] - 27s 426us/sample - loss: 1.0149 - accuracy: 0.5968 - val_loss: 0.8537 - val_accuracy: 0.6747\n",
      "Epoch 14/30\n",
      "64316/64316 [==============================] - 29s 445us/sample - loss: 0.9859 - accuracy: 0.6091 - val_loss: 0.8153 - val_accuracy: 0.6954\n",
      "Epoch 15/30\n",
      "64316/64316 [==============================] - 29s 448us/sample - loss: 0.9543 - accuracy: 0.6252 - val_loss: 0.7877 - val_accuracy: 0.7042\n",
      "Epoch 16/30\n",
      "64316/64316 [==============================] - 29s 448us/sample - loss: 0.9272 - accuracy: 0.6381 - val_loss: 0.7830 - val_accuracy: 0.7088\n",
      "Epoch 17/30\n",
      "64316/64316 [==============================] - 28s 443us/sample - loss: 0.9079 - accuracy: 0.6454 - val_loss: 0.7615 - val_accuracy: 0.7141\n",
      "Epoch 18/30\n",
      "64316/64316 [==============================] - 28s 436us/sample - loss: 0.8823 - accuracy: 0.6569 - val_loss: 0.7507 - val_accuracy: 0.7227\n",
      "Epoch 19/30\n",
      "64316/64316 [==============================] - 28s 435us/sample - loss: 0.8648 - accuracy: 0.6658 - val_loss: 0.7442 - val_accuracy: 0.7213\n",
      "Epoch 20/30\n",
      "64316/64316 [==============================] - 28s 432us/sample - loss: 0.8386 - accuracy: 0.6756 - val_loss: 0.7285 - val_accuracy: 0.7252\n",
      "Epoch 21/30\n",
      "64316/64316 [==============================] - 28s 434us/sample - loss: 0.8169 - accuracy: 0.6851 - val_loss: 0.7173 - val_accuracy: 0.7290\n",
      "Epoch 22/30\n",
      "64316/64316 [==============================] - 28s 432us/sample - loss: 0.8031 - accuracy: 0.6909 - val_loss: 0.7111 - val_accuracy: 0.7339\n",
      "Epoch 23/30\n",
      "64316/64316 [==============================] - 28s 443us/sample - loss: 0.7844 - accuracy: 0.6986 - val_loss: 0.6898 - val_accuracy: 0.7428\n",
      "Epoch 24/30\n",
      "64316/64316 [==============================] - 28s 439us/sample - loss: 0.7637 - accuracy: 0.7095 - val_loss: 0.7022 - val_accuracy: 0.7386\n",
      "Epoch 25/30\n",
      "64316/64316 [==============================] - 28s 433us/sample - loss: 0.7488 - accuracy: 0.7140 - val_loss: 0.6885 - val_accuracy: 0.7407\n",
      "Epoch 26/30\n",
      "64316/64316 [==============================] - 28s 431us/sample - loss: 0.7366 - accuracy: 0.7187 - val_loss: 0.7025 - val_accuracy: 0.7377\n",
      "Epoch 27/30\n",
      "64316/64316 [==============================] - 30s 472us/sample - loss: 0.7254 - accuracy: 0.7231 - val_loss: 0.7027 - val_accuracy: 0.7382\n",
      "Epoch 28/30\n",
      "64316/64316 [==============================] - 28s 440us/sample - loss: 0.7106 - accuracy: 0.7296 - val_loss: 0.6996 - val_accuracy: 0.7395\n"
     ]
    }
   ],
   "source": [
    "model_glove_bilstm = model_bi.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi.save('model_bilstm_a_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 10, 100)      737400      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 10, 100)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 10, 200)      121200      spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 200)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 400)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 5)            2005        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 860,605\n",
      "Trainable params: 123,205\n",
      "Non-trainable params: 737,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# main model\n",
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n",
    "x = SpatialDropout1D(0.2)(answer_emb1)\n",
    "x = Bidirectional(GRU(embedding_size_glove, return_sequences=True))(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "outp = Dense(5, activation=\"softmax\")(conc)\n",
    "    \n",
    "model_gru = Model(inputs=answer_inp, outputs=outp)\n",
    "model_gru.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "RocAuc = RocAucEvaluation(validation_data=(dev_a,dev_y), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98795 samples, validate on 19760 samples\n",
      "Epoch 1/30\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.672460 \n",
      "\n",
      "98795/98795 - 37s - loss: 1.4750 - accuracy: 0.3645 - val_loss: 1.4270 - val_accuracy: 0.3881\n",
      "Epoch 2/30\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.757883 \n",
      "\n",
      "98795/98795 - 35s - loss: 1.3814 - accuracy: 0.4194 - val_loss: 1.3081 - val_accuracy: 0.4523\n",
      "Epoch 3/30\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.818856 \n",
      "\n",
      "98795/98795 - 33s - loss: 1.2566 - accuracy: 0.4887 - val_loss: 1.1584 - val_accuracy: 0.5530\n",
      "Epoch 4/30\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.859144 \n",
      "\n",
      "98795/98795 - 33s - loss: 1.1424 - accuracy: 0.5469 - val_loss: 1.0397 - val_accuracy: 0.6025\n",
      "Epoch 5/30\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.884402 \n",
      "\n",
      "98795/98795 - 33s - loss: 1.0448 - accuracy: 0.5936 - val_loss: 0.9622 - val_accuracy: 0.6352\n",
      "Epoch 6/30\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.909178 \n",
      "\n",
      "98795/98795 - 33s - loss: 0.9663 - accuracy: 0.6264 - val_loss: 0.8410 - val_accuracy: 0.6883\n",
      "Epoch 7/30\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.922521 \n",
      "\n",
      "98795/98795 - 34s - loss: 0.9089 - accuracy: 0.6500 - val_loss: 0.7850 - val_accuracy: 0.7100\n",
      "Epoch 8/30\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.932917 \n",
      "\n",
      "98795/98795 - 34s - loss: 0.8588 - accuracy: 0.6719 - val_loss: 0.7306 - val_accuracy: 0.7339\n",
      "Epoch 9/30\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.941739 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.8158 - accuracy: 0.6899 - val_loss: 0.6727 - val_accuracy: 0.7584\n",
      "Epoch 10/30\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.947680 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.7802 - accuracy: 0.7036 - val_loss: 0.6478 - val_accuracy: 0.7698\n",
      "Epoch 11/30\n",
      "\n",
      " ROC-AUC - epoch: 11 - score: 0.955312 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.7570 - accuracy: 0.7122 - val_loss: 0.5913 - val_accuracy: 0.7906\n",
      "Epoch 12/30\n",
      "\n",
      " ROC-AUC - epoch: 12 - score: 0.958353 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.7272 - accuracy: 0.7237 - val_loss: 0.5808 - val_accuracy: 0.7920\n",
      "Epoch 13/30\n",
      "\n",
      " ROC-AUC - epoch: 13 - score: 0.961011 \n",
      "\n",
      "98795/98795 - 35s - loss: 0.7081 - accuracy: 0.7327 - val_loss: 0.5748 - val_accuracy: 0.7910\n",
      "Epoch 14/30\n",
      "\n",
      " ROC-AUC - epoch: 14 - score: 0.963913 \n",
      "\n",
      "98795/98795 - 33s - loss: 0.6867 - accuracy: 0.7416 - val_loss: 0.5383 - val_accuracy: 0.8079\n",
      "Epoch 15/30\n",
      "\n",
      " ROC-AUC - epoch: 15 - score: 0.966621 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.6724 - accuracy: 0.7464 - val_loss: 0.5170 - val_accuracy: 0.8140\n",
      "Epoch 16/30\n",
      "\n",
      " ROC-AUC - epoch: 16 - score: 0.970143 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.6554 - accuracy: 0.7529 - val_loss: 0.4814 - val_accuracy: 0.8319\n",
      "Epoch 17/30\n",
      "\n",
      " ROC-AUC - epoch: 17 - score: 0.970821 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.6443 - accuracy: 0.7581 - val_loss: 0.4868 - val_accuracy: 0.8243\n",
      "Epoch 18/30\n",
      "\n",
      " ROC-AUC - epoch: 18 - score: 0.973584 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.6283 - accuracy: 0.7630 - val_loss: 0.4515 - val_accuracy: 0.8435\n",
      "Epoch 19/30\n",
      "\n",
      " ROC-AUC - epoch: 19 - score: 0.974485 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.6185 - accuracy: 0.7673 - val_loss: 0.4465 - val_accuracy: 0.8423\n",
      "Epoch 20/30\n",
      "\n",
      " ROC-AUC - epoch: 20 - score: 0.975377 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.6141 - accuracy: 0.7697 - val_loss: 0.4528 - val_accuracy: 0.8400\n",
      "Epoch 21/30\n",
      "\n",
      " ROC-AUC - epoch: 21 - score: 0.977051 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.6042 - accuracy: 0.7727 - val_loss: 0.4348 - val_accuracy: 0.8476\n",
      "Epoch 22/30\n",
      "\n",
      " ROC-AUC - epoch: 22 - score: 0.977566 \n",
      "\n",
      "98795/98795 - 31s - loss: 0.5944 - accuracy: 0.7769 - val_loss: 0.4234 - val_accuracy: 0.8526\n",
      "Epoch 23/30\n",
      "\n",
      " ROC-AUC - epoch: 23 - score: 0.978248 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.5860 - accuracy: 0.7804 - val_loss: 0.4183 - val_accuracy: 0.8499\n",
      "Epoch 24/30\n",
      "\n",
      " ROC-AUC - epoch: 24 - score: 0.979987 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.5782 - accuracy: 0.7835 - val_loss: 0.4036 - val_accuracy: 0.8600\n",
      "Epoch 25/30\n",
      "\n",
      " ROC-AUC - epoch: 25 - score: 0.978729 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.5738 - accuracy: 0.7859 - val_loss: 0.4175 - val_accuracy: 0.8538\n",
      "Epoch 26/30\n",
      "\n",
      " ROC-AUC - epoch: 26 - score: 0.980717 \n",
      "\n",
      "98795/98795 - 34s - loss: 0.5609 - accuracy: 0.7893 - val_loss: 0.3965 - val_accuracy: 0.8590\n",
      "Epoch 27/30\n",
      "\n",
      " ROC-AUC - epoch: 27 - score: 0.982068 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.5548 - accuracy: 0.7913 - val_loss: 0.3727 - val_accuracy: 0.8715\n",
      "Epoch 28/30\n",
      "\n",
      " ROC-AUC - epoch: 28 - score: 0.982061 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.5516 - accuracy: 0.7951 - val_loss: 0.3747 - val_accuracy: 0.8691\n",
      "Epoch 29/30\n",
      "\n",
      " ROC-AUC - epoch: 29 - score: 0.981785 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.5481 - accuracy: 0.7941 - val_loss: 0.3909 - val_accuracy: 0.8632\n",
      "Epoch 30/30\n",
      "\n",
      " ROC-AUC - epoch: 30 - score: 0.984057 \n",
      "\n",
      "98795/98795 - 32s - loss: 0.5467 - accuracy: 0.7955 - val_loss: 0.3507 - val_accuracy: 0.8789\n"
     ]
    }
   ],
   "source": [
    "hist = model_gru.fit(train_a, train_y, batch_size=batch_size, epochs=epochs, validation_data=(dev_a, dev_y),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.save('model_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 10, 100)      737400      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 10, 100, 1)   0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 10, 1, 36)    3636        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 9, 1, 36)     7236        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 1, 36)     10836       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 1, 36)     18036       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 36)     0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 1, 36)     0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 144)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 144)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 5)            725         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 777,869\n",
      "Trainable params: 40,469\n",
      "Non-trainable params: 737,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 36\n",
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "x = Reshape((windows_size, embedding_size_glove, 1))(answer_emb1)\n",
    "maxpool_pool = []\n",
    "for i in range(len(filter_sizes)):\n",
    "  conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embedding_size_glove),kernel_initializer='he_normal', activation='elu')(x)\n",
    "  maxpool_pool.append(MaxPool2D(pool_size=(windows_size - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "z = Concatenate(axis=1)(maxpool_pool)   \n",
    "z = Flatten()(z)\n",
    "z = Dropout(0.1)(z)\n",
    "\n",
    "outp = Dense(5, activation=\"softmax\")(z)\n",
    "\n",
    "model = Model(inputs=answer_inp, outputs=outp)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98795 samples, validate on 19760 samples\n",
      "Epoch 1/30\n",
      "98795/98795 - 8s - loss: 1.4535 - accuracy: 0.3803 - val_loss: 1.3552 - val_accuracy: 0.4138\n",
      "Epoch 2/30\n",
      "98795/98795 - 8s - loss: 1.2482 - accuracy: 0.4899 - val_loss: 1.2066 - val_accuracy: 0.5063\n",
      "Epoch 3/30\n",
      "98795/98795 - 9s - loss: 1.0916 - accuracy: 0.5653 - val_loss: 1.0926 - val_accuracy: 0.5616\n",
      "Epoch 4/30\n",
      "98795/98795 - 9s - loss: 0.9770 - accuracy: 0.6166 - val_loss: 1.0281 - val_accuracy: 0.5894\n",
      "Epoch 5/30\n",
      "98795/98795 - 9s - loss: 0.8971 - accuracy: 0.6503 - val_loss: 0.9745 - val_accuracy: 0.6221\n",
      "Epoch 6/30\n",
      "98795/98795 - 9s - loss: 0.8382 - accuracy: 0.6755 - val_loss: 0.8786 - val_accuracy: 0.6598\n",
      "Epoch 7/30\n",
      "98795/98795 - 9s - loss: 0.7913 - accuracy: 0.6955 - val_loss: 0.8387 - val_accuracy: 0.6808\n",
      "Epoch 8/30\n",
      "98795/98795 - 9s - loss: 0.7545 - accuracy: 0.7111 - val_loss: 0.8131 - val_accuracy: 0.6906\n",
      "Epoch 9/30\n",
      "98795/98795 - 9s - loss: 0.7238 - accuracy: 0.7227 - val_loss: 0.8084 - val_accuracy: 0.6938\n",
      "Epoch 10/30\n",
      "98795/98795 - 9s - loss: 0.6981 - accuracy: 0.7325 - val_loss: 0.7770 - val_accuracy: 0.7036\n",
      "Epoch 11/30\n",
      "98795/98795 - 9s - loss: 0.6769 - accuracy: 0.7423 - val_loss: 0.7405 - val_accuracy: 0.7199\n",
      "Epoch 12/30\n",
      "98795/98795 - 9s - loss: 0.6563 - accuracy: 0.7482 - val_loss: 0.7537 - val_accuracy: 0.7135\n",
      "Epoch 13/30\n",
      "98795/98795 - 10s - loss: 0.6379 - accuracy: 0.7563 - val_loss: 0.6989 - val_accuracy: 0.7355\n",
      "Epoch 14/30\n",
      "98795/98795 - 10s - loss: 0.6252 - accuracy: 0.7618 - val_loss: 0.6820 - val_accuracy: 0.7433\n",
      "Epoch 15/30\n",
      "98795/98795 - 10s - loss: 0.6048 - accuracy: 0.7703 - val_loss: 0.7088 - val_accuracy: 0.7351\n",
      "Epoch 16/30\n",
      "98795/98795 - 10s - loss: 0.5960 - accuracy: 0.7729 - val_loss: 0.6610 - val_accuracy: 0.7517\n",
      "Epoch 17/30\n",
      "98795/98795 - 10s - loss: 0.5810 - accuracy: 0.7793 - val_loss: 0.6779 - val_accuracy: 0.7436\n",
      "Epoch 18/30\n",
      "98795/98795 - 10s - loss: 0.5732 - accuracy: 0.7829 - val_loss: 0.6591 - val_accuracy: 0.7529\n",
      "Epoch 19/30\n",
      "98795/98795 - 10s - loss: 0.5598 - accuracy: 0.7888 - val_loss: 0.6441 - val_accuracy: 0.7614\n",
      "Epoch 20/30\n",
      "98795/98795 - 10s - loss: 0.5499 - accuracy: 0.7898 - val_loss: 0.6207 - val_accuracy: 0.7713\n",
      "Epoch 21/30\n",
      "98795/98795 - 10s - loss: 0.5421 - accuracy: 0.7949 - val_loss: 0.6252 - val_accuracy: 0.7648\n",
      "Epoch 22/30\n",
      "98795/98795 - 10s - loss: 0.5350 - accuracy: 0.7965 - val_loss: 0.6062 - val_accuracy: 0.7739\n",
      "Epoch 23/30\n",
      "98795/98795 - 10s - loss: 0.5282 - accuracy: 0.7982 - val_loss: 0.6028 - val_accuracy: 0.7723\n",
      "Epoch 24/30\n",
      "98795/98795 - 10s - loss: 0.5154 - accuracy: 0.8054 - val_loss: 0.5785 - val_accuracy: 0.7849\n",
      "Epoch 25/30\n",
      "98795/98795 - 10s - loss: 0.5106 - accuracy: 0.8060 - val_loss: 0.6145 - val_accuracy: 0.7707\n",
      "Epoch 26/30\n",
      "98795/98795 - 10s - loss: 0.5045 - accuracy: 0.8085 - val_loss: 0.5997 - val_accuracy: 0.7757\n",
      "Epoch 27/30\n",
      "98795/98795 - 10s - loss: 0.4984 - accuracy: 0.8123 - val_loss: 0.5705 - val_accuracy: 0.7854\n",
      "Epoch 28/30\n",
      "98795/98795 - 10s - loss: 0.4906 - accuracy: 0.8149 - val_loss: 0.5626 - val_accuracy: 0.7907\n",
      "Epoch 29/30\n",
      "98795/98795 - 10s - loss: 0.4870 - accuracy: 0.8155 - val_loss: 0.5775 - val_accuracy: 0.7822\n",
      "Epoch 30/30\n",
      "98795/98795 - 11s - loss: 0.4823 - accuracy: 0.8175 - val_loss: 0.5710 - val_accuracy: 0.7879\n"
     ]
    }
   ],
   "source": [
    "model_cnn =  model.fit(train_a, train_y, batch_size=64, epochs=30, validation_data=(dev_a, dev_y), verbose=2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 10, 100)           737400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 6, 64)             32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 835,969\n",
      "Trainable params: 98,569\n",
      "Non-trainable params: 737,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_cnn = Sequential()\n",
    "lstm_cnn.add(Embedding(vocab_size+1, 100,weights=[embedding_matrix_lp],input_length=windows_size, trainable=False))\n",
    "lstm_cnn.add(Dropout(0.2))\n",
    "lstm_cnn.add(Conv1D(64, 5, activation='relu'))\n",
    "lstm_cnn.add(MaxPooling1D(pool_size=4))\n",
    "lstm_cnn.add(LSTM(100))\n",
    "lstm_cnn.add(Dense(5, activation='softmax'))\n",
    "lstm_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "lstm_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64316 samples, validate on 12864 samples\n",
      "Epoch 1/30\n",
      "64316/64316 [==============================] - 8s 132us/sample - loss: 1.5772 - accuracy: 0.2717 - val_loss: 1.5561 - val_accuracy: 0.2950\n",
      "Epoch 2/30\n",
      "64316/64316 [==============================] - 7s 109us/sample - loss: 1.5218 - accuracy: 0.3209 - val_loss: 1.5106 - val_accuracy: 0.3305\n",
      "Epoch 3/30\n",
      "64316/64316 [==============================] - 7s 108us/sample - loss: 1.4552 - accuracy: 0.3693 - val_loss: 1.4500 - val_accuracy: 0.3727\n",
      "Epoch 4/30\n",
      "64316/64316 [==============================] - 7s 108us/sample - loss: 1.4037 - accuracy: 0.3996 - val_loss: 1.4132 - val_accuracy: 0.4007\n",
      "Epoch 5/30\n",
      "64316/64316 [==============================] - 7s 110us/sample - loss: 1.3633 - accuracy: 0.4226 - val_loss: 1.3710 - val_accuracy: 0.4261\n",
      "Epoch 6/30\n",
      "64316/64316 [==============================] - 7s 110us/sample - loss: 1.3329 - accuracy: 0.4396 - val_loss: 1.3698 - val_accuracy: 0.4250\n",
      "Epoch 7/30\n",
      "64316/64316 [==============================] - 7s 111us/sample - loss: 1.3079 - accuracy: 0.4503 - val_loss: 1.3298 - val_accuracy: 0.4503\n",
      "Epoch 8/30\n",
      "64316/64316 [==============================] - 7s 110us/sample - loss: 1.2850 - accuracy: 0.4657 - val_loss: 1.3139 - val_accuracy: 0.4542\n",
      "Epoch 9/30\n",
      "64316/64316 [==============================] - 7s 110us/sample - loss: 1.2728 - accuracy: 0.4695 - val_loss: 1.2936 - val_accuracy: 0.4647\n",
      "Epoch 10/30\n",
      "64316/64316 [==============================] - 7s 111us/sample - loss: 1.2589 - accuracy: 0.4782 - val_loss: 1.2824 - val_accuracy: 0.4711\n",
      "Epoch 11/30\n",
      "64316/64316 [==============================] - 7s 115us/sample - loss: 1.2473 - accuracy: 0.4836 - val_loss: 1.2756 - val_accuracy: 0.4773\n",
      "Epoch 12/30\n",
      "64316/64316 [==============================] - 7s 111us/sample - loss: 1.2398 - accuracy: 0.4848 - val_loss: 1.2704 - val_accuracy: 0.4767\n",
      "Epoch 13/30\n",
      "64316/64316 [==============================] - 7s 106us/sample - loss: 1.2272 - accuracy: 0.4950 - val_loss: 1.2658 - val_accuracy: 0.4799\n",
      "Epoch 14/30\n",
      "64316/64316 [==============================] - 7s 107us/sample - loss: 1.2156 - accuracy: 0.5000 - val_loss: 1.2534 - val_accuracy: 0.4897\n",
      "Epoch 15/30\n",
      "64316/64316 [==============================] - 7s 106us/sample - loss: 1.2116 - accuracy: 0.4992 - val_loss: 1.2579 - val_accuracy: 0.4845\n",
      "Epoch 16/30\n",
      "64316/64316 [==============================] - 7s 107us/sample - loss: 1.2003 - accuracy: 0.5079 - val_loss: 1.2499 - val_accuracy: 0.4881\n",
      "Epoch 17/30\n",
      "64316/64316 [==============================] - 7s 108us/sample - loss: 1.1968 - accuracy: 0.5093 - val_loss: 1.2334 - val_accuracy: 0.4937\n",
      "Epoch 18/30\n",
      "64316/64316 [==============================] - 7s 106us/sample - loss: 1.1850 - accuracy: 0.5140 - val_loss: 1.2404 - val_accuracy: 0.4904\n",
      "Epoch 19/30\n",
      "64316/64316 [==============================] - 7s 108us/sample - loss: 1.1812 - accuracy: 0.5166 - val_loss: 1.2271 - val_accuracy: 0.5026\n",
      "Epoch 20/30\n",
      "64316/64316 [==============================] - 7s 108us/sample - loss: 1.1776 - accuracy: 0.5155 - val_loss: 1.2213 - val_accuracy: 0.5018\n",
      "Epoch 21/30\n",
      "64316/64316 [==============================] - 7s 109us/sample - loss: 1.1758 - accuracy: 0.5199 - val_loss: 1.2183 - val_accuracy: 0.5048\n",
      "Epoch 22/30\n",
      "64316/64316 [==============================] - 7s 106us/sample - loss: 1.1672 - accuracy: 0.5241 - val_loss: 1.2152 - val_accuracy: 0.5044\n",
      "Epoch 23/30\n",
      "64316/64316 [==============================] - 7s 107us/sample - loss: 1.1657 - accuracy: 0.5236 - val_loss: 1.2127 - val_accuracy: 0.5051\n",
      "Epoch 24/30\n",
      "64316/64316 [==============================] - 7s 108us/sample - loss: 1.1583 - accuracy: 0.5272 - val_loss: 1.2048 - val_accuracy: 0.5159\n",
      "Epoch 25/30\n",
      "64316/64316 [==============================] - 7s 107us/sample - loss: 1.1554 - accuracy: 0.5285 - val_loss: 1.2006 - val_accuracy: 0.5150\n",
      "Epoch 26/30\n",
      "64316/64316 [==============================] - 7s 106us/sample - loss: 1.1537 - accuracy: 0.5280 - val_loss: 1.1914 - val_accuracy: 0.5194\n",
      "Epoch 27/30\n",
      "64316/64316 [==============================] - 7s 107us/sample - loss: 1.1512 - accuracy: 0.5323 - val_loss: 1.1994 - val_accuracy: 0.5104\n",
      "Epoch 28/30\n",
      "64316/64316 [==============================] - 7s 108us/sample - loss: 1.1429 - accuracy: 0.5361 - val_loss: 1.1977 - val_accuracy: 0.5130\n",
      "Epoch 29/30\n",
      "64316/64316 [==============================] - 7s 106us/sample - loss: 1.1459 - accuracy: 0.5344 - val_loss: 1.2013 - val_accuracy: 0.5113\n"
     ]
    }
   ],
   "source": [
    "hist = lstm_cnn.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cnn.save('model_lstm_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(name='Attention_Weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='Attention_Bias',\n",
    "                                 shape=(input_shape[-1], ),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name='Attention_Context_Vector',\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x):\n",
    "        # refer to the original paper\n",
    "        # link: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "        \n",
    "        u_it = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        \n",
    "       #Through structure the vector applied  is used as\n",
    "        #Make attention value into probability distribution through\n",
    "        a_it = K.dot(u_it, self.u)\n",
    "        a_it = K.squeeze(a_it, -1)\n",
    "        a_it = K.softmax(a_it)\n",
    "        \n",
    "        return a_it\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_dim=50\n",
    "bilstm = Bidirectional(LSTM(lstm_dim, return_sequences=True))(answer_emb1)  #Input should be three dimensional\n",
    "# bilstm = LSTM(2*lstm_dim, return_sequences=True)(input_data) \n",
    "# bilstm_output = Dense(1)(bilstm)\n",
    "\n",
    "attention_layer = AttentionLayer()(bilstm)\n",
    "print(attention_layer)\n",
    "repeated_word_attention = RepeatVector(lstm_dim * 2)(attention_layer)\n",
    "repeated_word_attention = Permute([2, 1])(repeated_word_attention)\n",
    "sentence_representation = Multiply()([bilstm, repeated_word_attention])\n",
    "sentence_representation = Lambda(lambda x: K.sum(x, axis=1))(sentence_representation) #total summation of the multiplied bilstm and attention \n",
    "\n",
    "bilstm_output = Dense(5,activation='softmax')(sentence_representation)\n",
    "\n",
    "model = Model(inputs=[answer_inp],\n",
    "            outputs=[bilstm_output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_bilstm_attn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tensorflow_env': conda)",
   "name": "python_defaultSpec_1595524593312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
